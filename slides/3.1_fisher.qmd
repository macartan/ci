---
format: 
   revealjs:
    embed-resources: true
    theme: serif
    slide-level: 3
    slide-number: true
    show-slide-number: all
    preview-links: auto
    number-sections: true
    link-color: orange
title: "Design-based estimation"
author: "Macartan Humphreys"
bibliography: bib.bib
---

```{r, include = FALSE}

source("setup.R")

# run <- TRUE

```

# Frequentist Analysis {#secfisher}

## Basic Analysis {#nools}

-   Simple estimates from experimental data
-   Weighting, blocking
-   Design-based variance estimates
-   Design-based $p$ values
-   Controls and Doubly robust estimation
-   Reporting

### ATE: DIM

Unbiased estimates of the (sample) average treatment effect can be estimated (**whether or not there imbalance on covariates**) using:

$$
\widehat{ATE} = \frac{1}{n_T}\sum_TY_i - \frac{1}{n_C}\sum_CY_i,
$$

### ATE: DIM in practice {.smaller}

```{r}
df <- fabricatr::fabricate(N = 100, Z = rep(0:1, N/2), Y = rnorm(N) + Z)

# by hand
df |>
  summarize(Y1 = mean(Y[Z==1]), 
            Y0 = mean(Y[Z==0]), 
            diff = Y1 - Y0) |> kable(digits = 2)

# with estimatr
estimatr::difference_in_means(Y ~ Z, data = df) |>
  tidy() |> kable(digits = 2)
```

### ATE: DIM in practice  {.smaller}

We can also do this with regression:

```{r}
estimatr::lm_robust(Y ~ Z, data = df) |>
  tidy() |> kable(digits = 2)

```

See @freedman2008regression on why regression is fine here

### ATE: Blocks  {.smaller}

Say now different strata or blocks $\mathcal{S}$ had different *assignment probabilities*. Then you could estimate:

$$
\widehat{ATE} = \sum_{S\in \mathcal{S}}\frac{n_{S}}{n}
\left(\frac{1}{n_{S1}}\sum_{S\cap T}y_i - \frac{1}{n_{S0}}\sum_{S\cap C}y_i \right)
$$ 

**Note**: you cannot just ignore the blocks because assignment is no longer independent of potential outcomes: you might be sampling units with different potential outcomes with different probabilities.

However, the formula above works fine because selecting is random *conditional* on blocks.

### ATE: Blocks  {.smaller}

As a DAG this is just classic confounding:

```{r}
make_model("Block -> Z ->Y <- Block") |> 
  plot(x_coord = c(2,1,3), y_coord = c(2, 1, 1))
```

### ATE: Blocks in practice  {.smaller}

Data with heterogeneous assignments:

```{r}
df <- fabricatr::fabricate(
  N = 500, X = rep(0:1, N/2), 
  prob = .2 + .3*X,
  Z = rbinom(N, 1, prob),
  ip = 1/(Z*prob + (1-Z)*(1-prob)), # discuss
  Y = rnorm(N) + Z*X)
```

True effect is 0.5, but:

```{r}
estimatr::difference_in_means(Y ~ Z, data = df) |>
  tidy() |> kable(digits = 2)
```

### ATE: Blocks in practice  {.smaller}

Averaging over effects in blocks

```{r}
# by hand
estimates <- 
  df |>
  group_by(X) |>
  summarize(Y1 = mean(Y[Z==1]), 
            Y0 = mean(Y[Z==0]), 
            diff = Y1 - Y0,
            W = n())

estimates$diff |> weighted.mean(estimates$W)

# with estimatr
estimatr::difference_in_means(Y ~ Z, blocks = X, data = df) |>
  tidy() |> kable(digits = 2)
```

### ATE with IPW  {.smaller}

This also corresponds to the difference in the weighted average of treatment outcomes (with weights given by the inverse of the probability that each unit is assigned to treatment) and control outcomes (with weights given by the inverse of the probability that each unit is assigned to control).

-   The average difference in means estimator is the same as what you would get if you weighted inversely by shares of units in different conditions inside blocks.

### ATE with IPW in practice  {.smaller}

```{r}
# by hand
df |>
  summarize(Y1 = weighted.mean(Y[Z==1], ip[Z==1]), 
            Y0 = weighted.mean(Y[Z==0],  ip[Z==0]), # note !
            diff = Y1 - Y0)|> 
  kable(digits = 2)


# with estimatr
estimatr::difference_in_means(Y ~ Z, weights = ip, data = df) |>
  tidy() |> kable(digits = 2)
```

### ATE with IPW  {.smaller}

-   But **inverse propensity weighting** is a more general principle, which can be used even if you do not have blocks.

-   The intuition for it comes straight from **sampling weights** --- you weight up in order to recover an unbiased estimate of the potential outcomes for all units, whether or not they are assigned to treatment.

-   With sampling weights however you can include units even if their weight was 1. *Why can you not include these units when doing inverse propensity weighting?*

### Illustration: Estimating treatment effects with terrible treatment assignments: Fixer {.smaller}

Say you made a mess and used a randomization that was correlated with some variable, $U$. For example:

-   The randomization is done in a way that introduces a correlation between Treatment Assignment and Potential Outcomes
-   Then possibly, even though there is no true causal effect, we naively estimate a large one --- enormous bias
-   However since we know the assignment procedure we can **fully** correct for the bias

### Illustration: Estimating treatment effects with terrible treatment assignments: Fixer  {.smaller}

- In the next example, we do this using "**inverse propensity score weighting**." 
- This is exactly analogous to standard survey weighting --- since we selected different units for treatment with different probabilities, we weight them differently to recover the average outcome among treated units (same for control).

### Basic randomization: Fixer  {.smaller}

Bad assignment, some randomization process you can't understand (but can replicate) that results in unequal probabilities.


```{r}
N <- 400
U <- runif(N, .1, .9)

 design <- 
   declare_model(N = N,
                 Y_Z_0 = U + rnorm(N, 0, .1),
                 Y_Z_1 = U + rnorm(N, 0, .1),
                 Z = rbinom(N, 1, U)) +
  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +
  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +
  declare_estimator(Y ~ Z, label = "naive") 

```


### Basic randomization: Fixer  {.smaller}

Results is a sampling distribution not centered on the true effect (0)

```{r, echo = FALSE}
if(run)
  design |> diagnose_design(sims = 2000) |>write_rds("saved/fixer.rds")
diagnosis <- read_rds("saved/fixer.rds")
```

```{r, eval  = FALSE}
diagnosis <- diagnose_design(design)
```

```{r}
diagnosis$simulations_df |>
  ggplot(aes(estimate)) + geom_histogram() + facet_grid(~estimator) +
  geom_vline(xintercept = 0, color = "red")

```


### A fix  {.smaller}

To fix you can estimate the assignment probabilities by replicating the assignment many times:

```{r}
probs <- replicate(1000, design |> draw_data() |> pull(Z)) |> apply(1, mean)
```

and then use these assignment probabilities in your estimator

```{r}

design_2 <-
  design +
  declare_measurement(weights = Z/probs + (1-Z)/(1-probs)) +
  declare_estimator(Y ~ Z, weights = weights, label = "smart") 

```

### Basic randomization: Fixer  {.smaller}

Implied weights

```{r, echo = TRUE}

draw_data(design_2) |> 
  ggplot(aes(probs, weights, color = factor(Z))) + 
  geom_point()

```

### Basic randomization: Fixer {#fixer}

Improved results

```{r, echo = FALSE}
if(run)
  design_2 |> diagnose_design(sims = 1500) |>write_rds("saved/fixer2.rds")
diagnosis <- read_rds("saved/fixer2.rds")
```

```{r, eval  = FALSE}
diagnosis <- diagnose_design(design_2)
```

```{r}
diagnosis$simulations_df |>
  ggplot(aes(estimate)) + geom_histogram() + facet_grid(estimator~.)+
  geom_vline(xintercept = 0, color = "red")

```

### IPW with one unit! {.smaller}

This example is surprising but it helps you see the logic of why inverse weighting gets unbiased estimates (and why that might not guarantee a reasonable answer)

Imagine there is one unit with potential outcomes $Y(1) = 2, Y(0) = 1$. So the unit level treatment effect is 1.

You toss a coin.

-   If you assign to treatment you estimate: $\hat\tau = \frac{2}{0.5} = 4$
-   If you assign to control you estimate: $\hat\tau = -\frac{1}{0.5} = -2$

*So* your expected estimate is: $$0.5 \times 4 - 0.5 \times (-2) = 1$$

Great on average but *always* lousy


### Generalization: why IPW works {.smaller}


* Say a given unit is assigned to treatment with probability  $\pi_i$
* We estimate the average $Y(1)$ using 

$$\hat{\overline{Y_1}} = \frac{1}n\left(\sum_i \frac{Z_iY_i(1)}{\pi_i}\right)$$
With independent assignment the expected value of $\hat{\overline{Y_1}}$ is just:

$$\mathbb{E}[\hat{\overline{Y_1}}] =\frac1n\left( \left(\pi_1 \frac{1\times Y_1(1)}{\pi_1} + (1-\pi_1) \frac{0\times Y_1(1)}{\pi_1}\right) + \left(\pi_2 \frac{1\times Y_2(1)}{\pi_2} + (1-\pi_2) \frac{0\times Y_1(1)}{\pi_2}\right) + \dots\right)$$

$$\mathbb{E}[\hat{\overline{Y_1}}] =\frac1n\left( Y_1(1) + Y_2(1) + \dots\right) = \overline{Y_1}$$

and similarly for $\mathbb{E}[\hat{\overline{Y_0}}]$ and so using linearity of expectations:

$$\mathbb{E}[\widehat{\overline{Y_1 - Y_0}}] = \overline{Y_1 - Y_0}$$


### Generalization: why IPW works {.smaller}

* Note we needed $\pi_i >0$ and also $\pi_i <1$ everywhere. Why?
* We used independence here; sampling theory is used to show similar results for e.g. complete randomization
* For blocked randomization this is easy to see


## Design-based Estimation of Variance

*Lets talk about "inference"*

### Var(ATE)

-   Recall that the treatment effect is gotten by taking a sample of outcomes under treatment and comparing them to a sample of outcomes under control
-   Say that there is no "error"
-   Why would this procedure produce uncertainty?

### Var(ATE)

-   Why would this procedure produce uncertainty?
-   The uncertainty comes from being uncertain about the average outcome under control from observations of the control units, and from being uncertain about the average outcome under treatment from observation of the treated units
-   In other words, it comes from the variance in the treatment outcomes and variance in the control outcomes (and not, for example, from variance in the treatment effect)

### Var(ATE) {.smaller}

* In classical statistics we characterize our uncertainty over an estimate using an estimate of variance of the sampling distribution of the estimator. 

* Key idea is we want to be able to say: how likely are we to have gotten such an estimate if the distribution of estimates associated with our design looked a given way.

* More specifically we want to estimate "standard error" or the "standard deviation of the sampling distribution"

(See [Woolridge (2023)](https://www.sciencedirect.com/science/article/pii/S0304407623002336) where the standard error is understood as the  "*estimate* of the standard deviation of the sampling distribution")


### Variance and standard errors {.smaller}

Given:

* $\hat\tau$ is an estimate for $\tau$ 
* $\overline{x}$ is the average values of $x$ 

The variance of the estimator of $n$ repeated 'runs' of a design is:
$Var(\hat{\tau}) = \frac{1}n\sum_i(\hat\tau_i - \overline{\hat\tau_i})^2$

And the standard error is:


$se(\hat{\tau}) = \sqrt{\frac{1}n\sum_i(\hat\tau_i - \overline{\hat\tau_i})^2}$

### Variance and standard errors {.smaller}

If we have a good measure for the *shape* of the sampling distribution we can start to make statements of the form:

*  What are the chances that an estimate would be this large or larger?

If the sampling distribution is roughly normal, as it may be with large samples, then we can use procedures such as:  "there is a 5%  probability that an estimate would be more than 1.96 standard errors away from the mean of the sampling distribution"

### Var(ATE) {.smaller}

* **Key idea**: You can estimate variance straight from the data,  given knowledge of the **assignment process** and assuming well defined potential outcomes?

* Recall in general $Var(x) = \frac{1}n\sum_i(x_i - \overline{x})^2$. here the $x_i$s are the treatment effect estimates we might get under different random assignments, the $n$ is number of different assignments (assumed here all equally likely, but otherwise we can weight) and $\overline{x}$ is the truth.

* For intuition imagine we have just two units $A$, $B$, with potential outcomes $A_1$, $A_0$, $B_1$, $B_0$.

* When there are two units with outcomes $x_1, x_2$, the variance simplifies like this:

$$Var(x) = \frac{1}2\left(x_1 - \frac{x_1 + x_2}{2}\right)^2 + \frac{1}2\left(x_2 - \frac{x_1 + x_2}{2}\right)^2 = \left(\frac{x_1 - x_2}{2}\right)^2$$

### Var(ATE)  {.smaller}

In the two unit case the two possible treatment estimates are: $\hat{\tau}_1=A_1 - B_0$ and $\hat{\tau}_2=B_1 - A_0$, depending on what gets put into treatment. So the variance is:

$$Var(\hat{\tau}) = \left(\frac{\hat{\tau}_1 - \hat{\tau}_2}{2}\right)^2 = \left(\frac{(A_1 - B_0) - (B_1 - A_0)}{2}\right)^2 =\left(\frac{(A_1 - B_1) + (A_0 - B_0)}{2}\right)^2 $$
which we can re-write as:

$$Var(\hat{\tau}) =  \left(\frac{A_1 - B_1}{2}\right)^2 + \left(\frac{A_0 - B_0}{2}\right)^2+ 2\frac{(A_1 - B_1)(A_0-B_0)}{2}$$
The first two terms correspond to the variance of $Y(1)$ and the variance of $Y(0)$. The last term is a bit pesky though, it corresponds to twice the *covariance* of $Y(1)$ and $Y(0)$. 

### Var(ATE)  {.smaller}

How can we go about estimating this?

$$Var(\hat{\tau}) =  \left(\frac{A_1 - B_1}{2}\right)^2 + \left(\frac{A_0 - B_0}{2}\right)^2+ 2\frac{(A_1 - B_1)(A_0-B_0)}{2}$$

In the two unit case it is quite challenging because we do not have an estimate for any of the three terms: we do not have an estimate for the  variance in the treatment group or in the control group because we have only one observation in each case; and we do not have an estimate for the *covariance* because we don't observe both potential outcomes for *any* case.

Things do look a bit better however with more units...

### Var(ATE): Generalizing {.smaller}

From [Freedman Prop 1 / Example 1](http://www.stat.berkeley.edu/~census/neyregcm.pdf) (using combinatorics!) we have:

$V(\widehat{ATE}) = \frac{1}{n-1}\left[\frac{n_C}{n_T}V_1 + \frac{n_T}{n_C}V_0 + 2C_{01}\right]$

... where $V_0, V_1$ denote variances and $C_{01}$ covariance

This is usefully rewritten as:



$$
\begin{split}
  V(\widehat{ATE}) & = \frac{1}{n-1}\left[\frac{n - n_T}{n_T}V_1 + \frac{n - n_C}{n_C}V_0 + 2C_{01}\right] \\
    & = \frac{n}{n-1}\left[\frac{V_1}{n_T} + \frac{V_0}{n_C}\right] - \frac{1}{n-1}\left[V_1 + V_0 - 2C_{01}\right]
\end{split}
$$

where the final term is positive

### Var(ATE)

Note:

-   With more than two units we  cannow  use the sample estimates $s^2(\{Y_i\}_{i \in C})$ and $s^2(\{Y_i\}_{i \in T})$ for the first part.
-   But $C_{01}$ still cannot be estimated from data.
-   The **Neyman estimator** ignores the second part (and so is conservative).
-   Tip: for STATA users, use `, robust` (see @samii2012equivalencies)

### ATE and Var(ATE)

For the case with blocking, the conservative estimator is:

$V(\widehat{ATE}) = {\sum_{S\in \mathcal{S}}{\left(\frac{n_{S}}{n}\right)^2} \left({\frac{s^2_{S1}}{n_{S1}}} + {\frac{s^2_{S0}}{n_{S0}}} \right)}$

### Illustration of Neyman Conservative Estimator {.smaller}

An illustration of *how* conservative the conservative estimator of variance really is (numbers in plot are correlations between $Y(1)$ and $Y(0)$.

We confirm that:

1.  the estimator is conservative
2.  the estimator is more conservative for negative correlations between $Y(0)$ and $Y(1)$ --- eg if those cases that do particularly badly in control are the ones that do particularly well in treatment, and
3.  with $\tau$ and $V(Y(0))$ fixed, high positive correlations are associated with highest variance.

```{r, echo = FALSE}
n    <- 100; Y0 <- scale(1:n); e <- 2*abs(Y0)     
s    <- c(-1, -4/9, -1/9, .00001 , 1/9, 4/9, 1)   # Correlation between Y1,Y0
Z    <- c(rep(TRUE, n/2), rep(FALSE, n/2))
gY1  <- function(Y0, s) {.Y1 <- scale(sign(s)*( abs(s)^.5*Y0 + (1-abs(s))^.5*e))+1}
tau  <- sapply(s, function(i) mean(gY1(Y0, i) - Y0))
cors <- sapply(s, function(i) cor(gY1(Y0, i),  Y0))  # possible correlations
var1 <- sapply(s, function(i) var(gY1(Y0, i)))
phis <- sapply(s, function(i) (1/(n-1))*(2*cov(gY1(Y0, i), Y0) - var(gY1(Y0, i))-var(Y0)))  # Unknown term
tauhat <- function(sims, s=1){
 Y1 <- gY1(Y0, s)
 m  <- matrix(NA, sims)
  for(i in 1:sims){  Z=(1:100 %in% sample(n, n/2)); m[i]=mean(Y1[Z] - Y0[!Z])}
  m}
neyman <- function(sims, s=1, cons=1){  # Neyman Estimate
 Y1 <- gY1(Y0, s)   
 NE = matrix(NA, sims)
 for(i in 1:sims){ Z <- sample(Z)
 NE[i] <- (n/(n-1))*(var(Y1[Z])/(n/2) + var(Y0[!Z])/(n/2))+ 
 (1-cons)*(1/(n-1))*(2*cov(Y1, Y0) - var(Y1)-var(Y0))}
 mean(NE)}
V   <- sapply(s, function(i) var(tauhat(5000, i)))      # True variance; Empirical estimate
VN1 <- sapply(s, function(i) neyman(5000, i, cons=0)) # True variance; Formula check
VN2 <- sapply(s, function(i) neyman(5000, i, cons=1)) # Neyman conservative estimate
```


### Illustration of Neyman Conservative Estimator {.smaller}

| $\tau$ | $\rho$ | $\sigma^2_{Y(1)}$ | $\Delta$ | $\sigma^2_{\tau}$ | $\widehat{\sigma}^2_{\tau}$ | $\widehat{\sigma}^2_{\tau(\text{Neyman})}$ |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| 1.00   | -1.00  | 1.00              | -0.04    | 0.00              | -0.00                       | 0.04                                       |
| 1.00   | -0.67  | 1.00              | -0.03    | 0.01              | 0.01                        | 0.04                                       |
| 1.00   | -0.33  | 1.00              | -0.03    | 0.01              | 0.01                        | 0.04                                       |
| 1.00   | 0.00   | 1.00              | -0.02    | 0.02              | 0.02                        | 0.04                                       |
| 1.00   | 0.33   | 1.00              | -0.01    | 0.03              | 0.03                        | 0.04                                       |
| 1.00   | 0.67   | 1.00              | -0.01    | 0.03              | 0.03                        | 0.04                                       |
| 1.00   | 1.00   | 1.00              | 0.00     | 0.04              | 0.04                        | 0.04                                       |

Here $\rho$ is the unobserved correlation between $Y(1)$ and $Y(0)$; and $\Delta$ is the final term in the sample variance equation that we cannot estimate.

### Illustration of Neyman Conservative Estimator {.smaller}

```{r, echo = FALSE}
plot(V, VN1, xlim=c(0, max(VN1, VN2,V)), ylim=c(0, max(V, VN1, VN2)), 
main="Neyman estimator for  Y(1) and Y(0) correlations, ATE=1, Var(Y(0)=1, Var(Y(1) free" ,
ylab="Conservative Neyman Estimator of the Variance", xlab="True Variance of Estimates", col="grey")
lines(V, VN2, col="red"); text(V, VN2, round(cors,2), offset=TRUE); abline(0,1)
```

### Tighter Bounds On Variance Estimate 

The conservative variance comes from the fact that you do not know the covariance between $Y(1)$ and $Y(0)$.

-   But as [Aronow, Green, and Lee (2014)](http://arxiv.org/pdf/1405.6555.pdf) point out, you *do* know something.
-   Intuitively, if you know that the variance of $Y(1)$ is 0, then the covariance also has to be zero.
-   This basic insight opens a way of calculating bounds on the variance of the sample average treatment effect.

### Tighter Bounds On Variance Estimate {.smaller}

Example:

-   Take a million-observation dataset, with treatment randomly assigned
-   Assume $Y(0)=0$ for everyone and $Y(1)$ distributed normally with mean 0 and standard deviation of 1000.
-   Note here the covariance of $Y(1)$ and $Y(0)$ is 0.
-   Note the true variance of the estimated sample average treatment effect should be (approx) $\frac{Var(Y(1))}{{1000000}} + \frac{Var(Y(0))}{{1000000}} = 1+0=1$, for an se of $1$.
-   But using the Neyman estimator (or OLS!) we estimate (approx) $\frac{Var(Y(1))}{({1000000/2})} + \frac{Var(Y(0))}{({1000000/2})} = 2$, for an se of $\sqrt{2}$.
-   But we can recover the truth knowing the covariance between $Y(1)$ and $Y(0)$ is 0.

### Tighter Bounds On Variance Estimate: Code

```{r}
sharp_var <- function(yt, yc, N=length(c(yt,yc)), upper=TRUE){
  
  m <- length(yt)
  n <- m + length(yc)
  V <- function(x,N) (N-1)/(N*(length(x)-1)) * sum((x - mean(x))^2)
  yt <- sort(yt)
  if(upper) {yc <- sort(yc)
  } else {
    yc <- sort(yc,decreasing=TRUE)}
  p_i <- unique(sort(c(seq(0,n-m,1)/(n-m),seq(0,m,1)/m)))- 
        .Machine$double.eps^.5
  p_i[1] <- .Machine$double.eps^.5
  
  yti <- yt[ceiling(p_i*m)]
  yci <- yc[ceiling(p_i*(n-m))]
  
  p_i_minus <- c(NA,p_i[1: (length(p_i)-1)])
 
 ((N-m)/m * V(yt,N) + (N-(n-m))/(n-m)*V(yc,N) + 
     2*sum(((p_i-p_i_minus)*yti*yci)[2:length(p_i)]) - 2*mean(yt)*mean(yc))/(N-1)}
```

### Illustration

```{r}
n   <- 1000000
Y   <- c(rep(0,n/2), 1000*rnorm(n/2))
X   <- c(rep(0,n/2), rep(1, n/2))

lm_robust(Y~X) |> tidy() |> kable(digits = 2)

kable(t(as.matrix(ols)))

c(sharp_var(Y[X==1], Y[X==0], upper = FALSE),
  sharp_var(Y[X==1], Y[X==0], upper = TRUE)) |> 
  round(2)
```

The sharp bounds are $[1,1]$ but the conservative estimate is $\sqrt{2}$.

### Asymptotics

* It is a remarkable thing that you can estimate the standard error straight from the data
* However, once you want to *use* the standard error to do hypothesis testing you generally end up looking up distributions ($t$-distributions or normal distributions)
* That's a little disappointing and has been one of the criticisms made by @deaton2018understanding

However you *can* do hypothesis testing even without an estimate of the standard error.

Up next



## Randomization Inference {#ri}

*A procedure for using the randomization distribution to calculate $p$ values*

### Calculate a $p$ value in your head {.smaller}

-   Illustrating $p$ values via "randomization inference"

-   Say you randomized assignment to treatment and your data looked like this.

| Unit         | 1   | 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   | 10  |
|--------------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|
| Treatment    | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 1   | 0   | 0   |
| Health score | 4   | 2   | 3   | 1   | 2   | 3   | 4   | 8   | 7   | 6   |

Then:

-   Does the treatment improve your health?
-   What's the $p$ value for the null that treatment had no effect on anybody?

### Calculate a $p$ value in your head  {.smaller}

-   Illustrating $p$ values via "randomization inference"
-   Say you randomized assignment to treatment and your data looked like this.

| Unit         | 1   | 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   | 10  |
|--------------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|
| Treatment    | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 1   | 0   |
| Health score | 4   | 2   | 3   | 1   | 2   | 3   | 4   | 8   | 7   | 6   |

Then:

-   Does the treatment improve your health?
-   What's the $p$ value for the null that treatment had no effect on anybody?

### Randomization Inference: Some code

-   In principle it is very easy.
-   These few lines generate data, produce the regression estimate and then an `ri` estimate of $p$:

```{r}
# data
set.seed(1)
df <- fabricate(N = 1000, Z = rep(c(0,1), N/2), Y=  .1*Z + rnorm(N))

# test stat
test.stat <- function(df) with(df, mean(Y[Z==1])- mean(Y[Z==0]))

# test stat distribution
ts <- replicate(4000, df |> mutate(Z = sample(Z)) |> test.stat())

# test
mean(ts >= test.stat(df))   # One sided p value
```


### Randomization Inference: Some code

The $p$ value is the mass to the right of the vertical

```{r}
hist(ts); abline(v = test.stat(df), col = "red") 
```
### Using `ri2`

You can do the same using Alex Coppock's [ri2](https://cloud.r-project.org/web/packages/ri2/vignettes/ri2_vignette.html) package

```{r, eval = FALSE}
library(ri2)

# Declare the assignment
assignment <- declare_ra(N = 1000, m = 500)

# Implement
ri2_out <- conduct_ri(
  formula = Y ~ Z,
  declaration = assignment,
  sharp_hypothesis = 0,
  data = df, 
  p = "upper",
  sims = 4000
)

```


```{r, echo = FALSE}
library(ri2)

# Declare the assignment
assignment <- declare_ra(N = 1000, m = 500)

# Implement
if(run)
ri2_out <- conduct_ri(
  formula = Y ~ Z,
  declaration = assignment,
  sharp_hypothesis = 0,
  data = df, 
  p = "upper",
  sims = 4000
) |> write_rds("saved/ri2eg.rds")

```

### Using `ri2`

```{r, echo = FALSE}
read_rds("saved/ri2eg.rds") |> summary() |> kable()
```


You'll notice slightly different answer. This is because although the procedure is "exact" it is subject to simulation error.

### Randomization Inference {.smaller}

-   Randomization inference can get more complicated when you want to test a null other than the sharp null of no effect.
-   Say you wanted to test the null that the effect is 2 for all units. How do you do it?
-   Say you wanted to test the null that an *interaction effect* is zero. How do you do it?
-   In both cases by filling in a potential outcomes schedule given the hypothesis in question and then generating a test statistic

```{r,warning=FALSE, fig.align='center', echo = FALSE}

data <- data.frame(
  Observed_Y0 = c(1, 2, NA, NA),
  Observed_Y1 = c(NA, NA, 4, 3),
  UnderNull_0_Y0 = c(1, 2, 4, 3),
  UnderNull_0_Y1 = c(1, 2, 4, 3),
  UnderNull_2_Y0 = c(1, 2, 2, 1),
  UnderNull_2_Y1 = c(3, 4, 4, 3)
)

# Custom labels
custom_labels <- c("Y(0)", "Y(1)", "Y(0)", "Y(1)", "Y(0)", "Y(1)")

# Rename columns before applying column_spec
data_renamed <- data
colnames(data_renamed) <- custom_labels

kable(data_renamed, format = "html", table.attr = "class='table'", row.names = FALSE) %>%
  kable_styling(full_width = FALSE) %>%
  add_header_above(c("Observed" = 2, "Under null that\n effect is 0" = 2, "Under null that\n effect is 2" = 2))


```


### `ri` and CIs {.smaller}

It is possible to use this procedure to generate **confidence intervals** with a natural interpretation.

* The key idea is that we can use the same procedure to assess the probability of the data given a sharp null of no effect, but also a sharp null of *any other* **constant* effect. 
* We can then see what set of effects we *reject* and what set we *accept* 
* We are left with a set of values that *we cannot reject* at the 0.05 level.


### `ri` and CIs in practice  {.smaller}

```{r, eval = FALSE}
candidates <- seq(-.05, .3, length = 50)

get_p <- function(j)
  (conduct_ri(
      formula = Y ~ Z,
      declaration = assignment,
      sharp_hypothesis = j,
      data = df,
      sims = 5000,
    ) |> summary()
  )$two_tailed_p_value

# Implement
ps <- candidates |> sapply(get_p)
```

### `ri` and CIs in practice  {.smaller}

```{r, echo = FALSE}
candidates <- seq(-.05, .3, length = 40)

get_p <- function(j)
  (conduct_ri(
      formula = Y ~ Z,
      declaration = assignment,
      sharp_hypothesis = j,
      data = df,
      sims = 5000,
    ) |> summary()
  )$two_tailed_p_value

# Implement
if(run)
 candidates |> sapply(get_p) |>
  write_rds("saved/ri_ci.rds")

ci_df <- 
  data.frame(candidates = candidates, p = read_rds("saved/ri_ci.rds"))  


```

```{r, echo = FALSE}
ci_df |>
  ggplot(aes(candidates, p)) + geom_line() + geom_point() + theme_bw() +
  geom_hline(aes(yintercept = .05), color = "orange")
```

**Warning**: calculating confidence intervals this way can be computationally intensive


### `ri` with `DeclareDesign`  {.smaller}

* `DeclareDesign` can do randomization inference natively. 
* The trick is to ensure that the *only* stochastic component is the assignment to treatment when calculating the $p$ values 
* (though if you are very careful you can also think through other sources of variability).

```{r}
set.seed(1)
df <- fabricate(N = 1000, Z = rep(c(0,1), N/2), Y=  .1*Z + rnorm(N))
my_estimate <- with(df, mean(Y[Z==1]) - mean(Y[Z==0]))

design <- 
  declare_model(data = df) +
  declare_measurement(Z = sample(Z)) +
  declare_estimator(Y ~ Z)
```

### `ri` with `DeclareDesign` {.smaller}

```{r, eval = FALSE}

ri_ps <-  function(my_estimate) 
  declare_diagnosands(
    one_sided_pos = mean(estimate >= my_estimate),
    one_sided_neg = mean(estimate <= my_estimate),
    two_sided = mean(abs(estimate) >= abs(my_estimate)))

diagnose_design(design, sims = 1000, diagnosands = ri_ps(my_estimate))

```

```{r, echo = FALSE}
ri_ps <-  function(my_estimate) 
  declare_diagnosands(
    one_sided_pos = mean(estimate >= my_estimate),
    one_sided_neg = mean(estimate <= my_estimate),
    two_sided = mean(abs(estimate) >= abs(my_estimate)))

if(run)
  diagnose_design(design, sims = 1000, diagnosands = ri_ps(my_estimate)) |>
  write_rds("saved/ri_dd.rds")

read_rds("saved/ri_dd.rds") |> reshape_diagnosis() |> kable()

```


### `ri` interactions {.smaller}

Lets now imagine a world with two treatments and we are interested in using `ri` for assessing the interaction. (Code from [Coppock, `ri2`](https://cloud.r-project.org/web/packages/ri2/vignettes/ri2_vignette.html)) 

```{r}
set.seed(1)

N <- 100
declaration <- randomizr::declare_ra(N = N, m = 50)

data <- fabricate(
  N = N,
  Z = conduct_ra(declaration),
  X = rnorm(N),
  Y = .9 * X + .2 * Z + .1 * X * Z + rnorm(N))
```

### `ri` interactions {.smaller}

The approach is to declare a null model that is nested by the full model. Then $F$ test statistic from the model comparisons is taken as the test statistic and distribution of this is built up under re-randomizations.

```{r}
conduct_ri(
    model_1 = Y ~ Z + X,
    model_2 = Y ~ Z + X + Z * X,
    declaration = declaration,
    assignment = "Z",
    sharp_hypothesis = coef(lm(Y ~ Z, data = data))[2],
    data = data, 
    sims = 1000
  )  |> summary() |> kable()
```

### `ri` interactions with `DeclareDesign` {.smaller}

Let's imagine a true model with interactions. We take an estimate. We then ask how likely that estimate is from a *null* model with constant effects

Note: this is *quite* a sharp hypothesis

```{r}
df <- fabricate(N = 1000, Z1 = rep(c(0,1), N/2), Z2 = sample(Z1), Y = Z1 + Z2 - .15*Z1*Z2 + rnorm(N))

my_estimate <- (lm(Y ~ Z1*Z2, data = df) |> coef())[4]

null_model <-  function(df) {
  M0 <- lm(Y ~ Z1 + Z2, data = df) 
  d1 <- coef(M0)[2]
  d2 <- coef(M0)[3]
  df |> mutate(
    Y_Z1_0_Z2_0 = Y - Z1*d1 - Z2*d2,
    Y_Z1_1_Z2_0 = Y + (1-Z1)*d1 - Z2*d2,
    Y_Z1_0_Z2_1 = Y - Z1*d1 + (1-Z2)*d2,
    Y_Z1_1_Z2_1 = Y + (1-Z1)*d1 + (1-Z2)*d2)
  }
```

### `ri` interactions with `DeclareDesign` {.smaller}

Let's imagine a true model with interactions. We take an estimate. We then ask how likely that estimate is from a *null* model with constant effects

**Imputed** potential outcomes  look like this:

```{r}
df <- df |> null_model()

df |>  head() |> kable(digits = 2, align = "c")
```

### `ri` interactions with `DeclareDesign` {.smaller}


```{r}
design <- 
  declare_model(data = df) +
  declare_measurement(Z1 = sample(Z1), Z2 = sample(Z2),
                      Y = reveal_outcomes(Y ~ Z1 + Z2)) +
  declare_estimator(Y ~ Z1*Z2, term = "Z1:Z2")
```


```{r, eval = FALSE}
diagnose_design(design, sims = 1000, diagnosands = ri_ps(my_estimate))
```


```{r, echo = FALSE}
if(run)
  diagnose_design(design, sims = 1000,
                diagnosands = ri_ps(my_estimate)) |>
  write_rds("saved/ri_dd_inter.rds")

read_rds("saved/ri_dd_inter.rds") |> reshape_diagnosis() |> kable()

```




### `ri` in practice

* In practice (unless you have a design declaration), it is a good idea to create a $P$ matrix when you do your randomization. 
* This records the set of possible randomizations you might have had: or a sample of these.
* So, again: assignments have to be replicable

### `ri` Applications

- Recall that  silly randomization procedure from [this slide](#fixer). 
- Say you forgot to take account of the wacky assignment in your estimates and you estimate 0.15.
- *Does the treatment improve your health?*: $p=?$



```{r, echo = FALSE, fig.height = 3, fig.width = 8}
diagnosis <- read_rds("saved/fixer2.rds")
diagnosis$simulations_df |> filter(estimator == "naive") |>
  ggplot(aes(estimate)) + geom_histogram() +
  geom_vline(xintercept = 0, color = "red")

```




### `ri` Applications

-   Randomization procedures are sometimes funky in lab experiments
-   Using randomization inference would force a focus on the true assignment of individuals to treatments
-   Fake (but believable) example follows

### `ri` Applications {.smaller}
 
::: {fig-optimalassigment}
|         |          | Capacity | T1  | T2  | T3  |
|---------|----------|----------|-----|-----|-----|
| Session | Thursday | 40       | 10  | 30  | 0   |
|         | Friday   | 40       | 10  | 0   | 30  |
|         | Saturday | 10       | 10  | 0   | 0   |

Optimal assignment to treatment given constraints due to facilities
:::

::: {fig-constraints}
| Subject type | N   | Available  |
|--------------|-----|------------|
| A            | 3   | Thurs, Fri |
| B            | 30  | Thurs, Sat |
| C            | 30  | Fri, Sat   |

Constraints due to subjects
:::

### `ri` Applications {.smaller}

If you think hard about assignment you might come up with an allocation like this.

```{r, warning=FALSE, echo=FALSE,fig.align='center', fig.cap="Assignment of people to days"}
df <- data.frame(
  Subject_Type = c("A", "B", "C"),
  N = c(30, 30, 30),
  Available = c("Thurs, Fri", "Thurs, Sat", "Fri, Sat"),
  Thurs = c(15, 25, NA),
  Fri = c(15, NA, 25),
  Sat = c(NA, 5, 5)
)

# Custom labels
custom_labels <- c("Subject type", "N", "Available", "Thurs", "Fri", "Sat")

# Rename columns before applying column_spec
colnames(df) <- custom_labels

kable(df, format = "html", table.attr = "class='table'", row.names = FALSE) %>%
  kable_styling(full_width = FALSE) %>%
  add_header_above(c(" " = 3, "Allocations" = 3))
```

### `ri` Applications {.smaller}

That allocation balances as much as possible. Given the allocation you might randomly assign individuals to different days as well as randomly assigning them to treatments within days. If you then figure out assignment propensities, this is what you would get:

```{r, warning=FALSE, echo=FALSE,fig.align='center'}
df <- data.frame(
  Subject_Type = c("A", "B", "C"),
  N = c(30, 30, 30),
  Available = c("Thurs, Fri", "Thurs, Sat", "Fri, Sat"),
  T1 = c(0.25, 0.375, 0.375),
  T2 = c(0.375, 0.625, NA),  # Use NA for missing value
  T3 = c(0.375, 0, 0.625)
)

# Custom labels
custom_labels <- c("Subject type", "N", "Available", "T1", "T2", "T3")

# Rename columns before applying column_spec
colnames(df) <- custom_labels

kable(df, format = "html", table.attr = "class='table'", row.names = FALSE) %>%
  kable_styling(full_width = FALSE) %>%
  add_header_above(c(" " = 3, "Assignment Probabilities" = 3))
```

### `ri` Applications {.smaller}

[Even under the assumption that the day of measurement does not matter, these assignment probabilities have big implications for analysis.]{style="font-size: 80%;"}

```{r,warning=FALSE, echo=FALSE, fig.align='center'}
df <- data.frame(
  Subject_Type = c("A", "B", "C"),
  N = c(30, 30, 30),
  Available = c("Thurs, Fri", "Thurs, Sat", "Fri, Sat"),
  T1 = c(0.25, 0.375, 0.375),
  T2 = c(0.375, 0.625, NA),  # Use NA for missing value
  T3 = c(0.375, 0, 0.625)
)

# Custom labels
custom_labels <- c("Subject type", "N", "Available", "T1", "T2", "T3")

# Rename columns before applying column_spec
colnames(df) <- custom_labels

kable(df, format = "html", table.attr = "class='table'", row.names = FALSE) %>%
  kable_styling(full_width = FALSE) %>%
  add_header_above(c(" " = 3, "Assignment Probabilities" = 3))
```

-   Only the type $A$ subjects could have received any of the three treatments.

-   There are no two treatments for which it is possible to compare outcomes for subpopulations $B$ and $C$

-   A comparison of $T1$ versus $T2$ can only be made for population $A \cup B$

-   However subpopulation $A$ is assigned to $A$ (versus $B$) with probability 4/5; while population $B$ is assigned with probability 3/8

### `ri` Applications {.smaller}

-   **Implications for design**: need to uncluster treatment delivery

-   **Implications for analysis**: need to take account of propensities

**Idea**: Wacky assignments happen but if you know the propensities you can do the analysis.


### `ri` Applications: Indirect assignments {.smaller}


A particularly interesting application is where a random assignment combines with existing features to determine an assignment to an "indirect" treatment.

* For instance:  $n$ of $N$ are assigned to a treatment.
* You are interested in whether "having a friend assigned to treatment" makes a difference to a subject.  Or maybe "a friend of a friend"
* That means the subject has a complex *clustered* assignment that depends on how many friends they have
* A bit mind-boggling, but:
  * Rerun your assignment many times and each time figure out whether a subject is assigned to an indirect treatment or not 
  * Calculate the implied quantity of interest for each assignment
  * Assess the place of the actual quantity in the sampling distribution

## Covariate Adjustment


### Example {.smaller}

Consider for example this data.

-   You randomly pair offerers and receivers in a dictator game (in which offerers decide how much of \$1 to give to receivers).
-   Your population comes from two groups (80% Baganda and 20% Banyankole) *so in randomly assigning partners you are randomly determining whether a partner is a coethnic or not*.
-   **You find that in non-coethnic pairings 35% is offered, in coethnic pairings 48% is offered.**

Should you believe it?

### Covariate Adjustment {.smaller}

-   Population: randomly matched Baganda (80% of pop) and Banyankole (20% of pop)
-   You find: in non-coethnic pairings 35% is offered, in coethnic pairings 48% is offered.
-   But a closer look at the data reveals...

::: {#fig-games}
|           |            | To: Baganda | To: Banyankole |
|-----------|------------|-------------|----------------|
| Offers by | Baganda    | 64%         | 16%            |
|           | Banyankole | 16%         | 4%             |

Number of Games
:::

::: {#fig-offers}
|           |            | To: Baganda | To: Banyankole |
|-----------|------------|-------------|----------------|
| Offers by | Baganda    | 50          | 50             |
|           | Banyankole | 20          | 20             |

Average Offers
:::

So that's a problem

### Covariate Adjustment

Control?

-   With such data you might be tempted to 'control' for the covariate (here: ethnic group), using regression.
-   But, perhaps surprisingly, it turns out that regression with covariates does not estimate average treatment effects.
-   It does estimate an average of treatment effects, but specifically a minimum variance estimator, not necessarily an estimator of your estimand.

### Covariate Adjustment

Compare:

-   $\hat{\tau}_{ATE} =\sum_{x} \frac{w_x}{\sum_{j}w_{j}}\hat{\tau}_x$
-   $\hat{\tau}_{OLS} =\sum_{x} \frac{w_xp_x(1-p_x)}{\sum_{j}w_j{p_j(1-p_j)}}\hat{\tau}_x$

Instead you can use formula above for $\hat{\tau}_{ATE}$ to estimate ATE

alternatively...

### Covariate adjustment via saturated regression {.smaller}

* Alternatively you can use propensity weights.
* Alternatively you can use a regression that includes both the treatment and the treatment *interacted* with the covariates.
  * In practice this is best done by *demeaning* the covariates; doing this lets you read off the average effect from the main term. Key resource: @lin2012agnostic

You should have noticed that the logic for controlling for a covariate here is equivalent to the logic we saw for heterogeneous assignment propensities. These are really the same thing.

### Covariate adjustment via saturated regression {.smaller}

Returning to prior example:

```{r}
df <- fabricatr::fabricate(
  N = 500, 
  X = rep(0:1, N/2), 
  Z = rbinom(N, 1, .2 + .3*X),
  Y = rnorm(N) + Z*X)

lm_robust(Y ~ Z*X_c, data = df |> mutate(X_c = X - mean(X))) |>
  tidy() |> kable(digits = 2)
```

### Covariate adjustment via saturated regression {.smaller}

Returning to prior example:

```{r}

lm_lin(Y ~ Z, ~ X, data = df) |>
  tidy() |> kable(digits = 2)

```

### Demeaning and saturating 

Demeaning interactions

-   Say you have a factorial design with treatments X1 and X2 (or observational data with two covariates)
-   You analyse with a model that has main terms and interaction terms
-   Interpreting coefficients can be confusing, but sometimes demeaning can help. What does demeaning do?


### Demeaning and saturating {.smaller}

Let's:

-   Declare a factorial design in which Y is generated according to

`f_Y <- function(X1, X2, u) .1 + .2*X1 + .3*X2 + u*X1*X2`

where u is distributed $U[0,1]$.

-   Specify estimands carefully
-   Run analyses in which we do and do not demean the treatments; compare and explain results

### Demeaning interactions {.smaller}

```{r}
f_Y <- function(X1, X2, u) .1 + .2*X1 + .3*X2 + u*X1*X2

design <-
  declare_model(N = 1000, u = runif(N),
                X1 = complete_ra(N), X2 = block_ra(blocks = X1),
                X1_demeaned = X1 - mean(X1),
                X2_demeaned = X2 - mean(X2),
                Y = f_Y(X1, X2, u)) +
  declare_inquiry(
    base = mean(f_Y(0, 0, u)),
    average = mean(f_Y(0, 0, u) + f_Y(0, 1, u)  + f_Y(1, 0, u)  + f_Y(1, 1, u))/4,
    CATE_X1_given_0 = mean(f_Y(1, 0, u) - f_Y(0, 0, u)),
    CATE_X2_given_0 = mean(f_Y(0, 1, u) - f_Y(0, 0, u)),
    ATE_X1 = mean(f_Y(1, X2, u) - f_Y(0, X2, u)),
    ATE_X2 = mean(f_Y(X1, 1, u) - f_Y(X1, 0, u)),
    I_X1_X2 = mean((f_Y(1, 1, u) - f_Y(0, 1, u)) - (f_Y(1, 0, u) - f_Y(0, 0, u)))
  ) +
  declare_estimator(Y ~ X1*X2, 
                    inquiry = c("base", "CATE_X1_given_0", "CATE_X2_given_0", "I_X1_X2"), 
                    term = c("(Intercept)", "X1", "X2", "X1:X2"),
                    label = "natural") +
  declare_estimator(Y ~ X1_demeaned*X2_demeaned, 
                    inquiry = c("average", "ATE_X1", "ATE_X2", "I_X1_X2"), 
                    term = c("(Intercept)", "X1_demeaned", "X2_demeaned", "X1_demeaned:X2_demeaned"),
                    label = "demeaned")
```

### Demeaning interactions: Solution {.smaller}

```{r, echo = FALSE}
if(run)
  design |> diagnose_design(bootstrap_sims = 0) |>write_rds("saved/demeaned.rds")

read_rds("saved/demeaned.rds") |> reshape_diagnosis() |> 
  select(Estimator,	 Inquiry,	Term,	'Mean Estimand',	'Mean Estimate') |>  
  arrange(Estimator, Inquiry) |>
  kable(align = c("l", "l", "l", "c", "c"))

```

It's all good. But you need to match the estimator to the inquiry: demean for average marginal effects; do not demean for conditional marginal effects.

### Recap {.smaller}

If you have different groups with different assignment propensities you can do any or all of these:

1.  Blocked differences in means
2.  Inverse propensity weighting
3.  Saturated regression (Lin)
4.  More... (coming)

You cannot (reliably):

1.  Ignore the groups
2.  Include them in a regression (without interactions)

### Covariate Adjustment: Wilder covariates {#CA}  {.smaller}

- Even though randomization ensures no bias, you may sometimes **want** to "**control**" for covariates in order to improve efficiency (see the discussion of blocking above).
- Or you may **have** to take account of the fact that the assignment to treatment is correlated with a covariate (as above).
- In observational work you might also figure out you have to control for a covariate to justify inferences (Refer to our discussion of the backdoor criteria)

### Conditional Bias and Precision Gains from Controls

* Experimental motivation: Controls can  reduce noise and improve precision. This is an argument for using variables that are correlated with the output (not with the treatment).
* Observational motivation: Controls can provide grounds for identification

### How much gain from controls?

```{r, echo = FALSE, fig.cap = "Gains as a function of the correlation of a control and an outcome"}
knitr::include_graphics("figs/n.png")
```

### Precision Gains from Controls  {.smaller}

Introducing controls can create complications

* As argued by Freedman (summary from @lin2012agnostic), we can get: "worsened asymptotic precision, invalid measures of precision, and small-sample bias"$^*$

* These adverse effects are essentially removed with an interacted model

* See discussions in @imbens2015causal (7.6, 7.7) and especially Theorem 7.2 for the asymptotic variance of the estimator

$^*$ though note that the precision concern does not hold when treatment and control groups are equally sized

### To control or not {.smaller}

We will illustrate by comparing:

  * DIM
  * OLS (linear controls)
  * Lin (saturated)

With:

  * Varying fraction assigned to treatment
  * Varying relation between $Y(0)$ and $Y(1)$

### Declaration (from RDSS) {.smaller}

```{r}
# https://book.declaredesign.org/library/experimental-causal.html

prob <- 0.5
control_slope <- -1

declaration_18.3 <-
  declare_model(N = 100, X = runif(N, 0, 1),
                U = rnorm(N, sd = 0.1),
                Y_Z_1 = 1*X + U, Y_Z_0 = control_slope*X + U
  ) +
  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +
  declare_assignment(Z = complete_ra(N = N, prob = prob)) + 
  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +
  declare_estimator(Y ~ Z, inquiry = "ATE", label = "DIM") +
  declare_estimator(Y ~ Z + X, .method = lm_robust, inquiry = "ATE", label = "OLS") +
  declare_estimator(Y ~ Z, covariates = ~X, .method = lm_lin, 
                    inquiry = "ATE", label = "Lin")
```

### Implied potential outcomes

The variances and covariance of potential outcomes depend on the slope parameter

```{r, echo = FALSE, fig.width = 8, fig.height = 3}  
datas <- declaration_18.3 |> 
  redesign(control_slope = seq(-1, 1, 1)) |> 
  lapply(function(x) draw_data(x) )   

names(datas) <- paste("slope", -1:1)
  
datas |>    bind_rows(.id = "control_slope") |>
  gather(var, val, -X,  -Y, -Z, -ID, -U, -control_slope) |>
  ggplot(aes(X, val, color = var)) + geom_point(alpha = .5) + 
  facet_grid(~control_slope) + theme_bw()

```

### To control or not {.smaller}

Diagnosis:

```{r, eval = FALSE}
declaration_18.3 |> 
  redesign(
    control_slope = seq(-1, 1, 0.5), 
    prob = seq(0.1, 0.9, 0.1)) |> 
  diagnose_designs()
```

### To control or not {.smaller}

* Lin always does well
* OLS does fine with equal probability assignments
* Otherwise the ranking of DIM and OLS is design dependent

```{r, echo = FALSE, fig.width = 8, fig.height = 3}
if(run)
  declaration_18.3 |> 
  redesign(
    control_slope = seq(-1, 1, 0.5), 
    prob = seq(0.1, 0.9, 0.1)) |> 
  diagnose_designs() |>
  write_rds("saved/freedman18.3.rds") 

read_rds("saved/freedman18.3.rds")$diagnosands_df |> 
  ggplot(aes(prob, sd_estimate, color = estimator)) + geom_line() + facet_grid(~control_slope, labeller = label_both) + theme_bw() +
  xlab("Fraction assigned to treatment") + ylab("(real) standard error")

       
         
```

### Conditional Bias and Precision Gains from Controls

Treatment  correlated with covariates can induce "conditional bias." Including controls can change your estimates so be sure not to fish!

```{r, echo = FALSE, fig.cap = "Advantages of controlling for vars that are correlated with outcomes"}
knitr::include_graphics("figs/cb.jpg")
```



## Doubly robust estimation

### Doubly robust estimation {.smaller}

Doubly robust estimation combines:

1.  A model for how the covariates predict the potential outcomes
2.  A model for how the covariates predict assignment propensities

Using both together to estimate potential outcomes using propensity weighting lets you do well even if either model is wrong.

Each part can be done using nonparameteric methods resulting in an overall semi-parametric procedure.

-   $\pi(Z) = \Pr(Z=1|X)$: Estimate $\hat\pi$
-   $Y_z = \mathbb{E}[Y|Z=z, X]$: Estimate $\hat{Y}_z$
-   Estimate of causal effect: $\frac{1}{n}\sum_{i=1}^n\left(\left(\frac{Z_i}{\hat{\pi}_i}(Y_i - \hat{Y}_{i1}\right) - \left(\frac{1-Z_i}{1-\hat{\pi}_i}(Y_i - \hat{Y}_{i0}\right) + \left(\hat{Y}_{i1} - \hat{Y}_{i0}\right) \right)$

### Doubly robust estimation

-   Estimate of causal effect: $\frac{1}{n}\sum_{i=1}^n\left(\left(\frac{Z_i}{\hat{\pi}_i}(Y_i - \hat{Y}_{i1}\right) - \left(\frac{1-Z_i}{1-\hat{\pi}_i}(Y_i - \hat{Y}_{i0}\right) + \left(\hat{Y}_{i1} - \hat{Y}_{i0}\right) \right)$

-   Note that if $\hat{Y}_{iz}$ are correct then the first parts drop out and we we get the right answer.

-   So if you can impute the potential outcomes, you are good (though hardly surprising)

### Doubly robust estimation {.smaller}

-   More subtly say the $\hat{\pi}$s are correct, but your imputations are wrong; then we again have an unbiased estimator.

To see this imagine with probability $\pi$ we assign unit 1 to treatment and 2 to control (otherwise 1 to control and 2 to treatment).

Then our *expected* estimate is:

$\frac12\pi\left(\left(\frac{1}{\pi}(Y_{11} - \hat{Y}_{11}\right) - \left(\frac{1}{\pi}(Y_{20} - \hat{Y}_{20}\right) \right) + (1-\pi)\left(\left(\frac{1}{1-\pi}(Y_{21} - \hat{Y}_{21}\right) - \left(\frac{1}{1-\pi}(Y_{10} - \hat{Y}_{10}\right) \right) + \left(\hat{Y}_{11} - \hat{Y}_{20}\right) + \left(\hat{Y}_{21} - \hat{Y}_{10}\right)$

$\frac12\left(Y_{11} - Y_{10} + Y_{21}- Y_{20} +\pi\left(\left(\frac{1}{\pi}( - \hat{Y}_{11}\right) - \left(\frac{1}{\pi}( - \hat{Y}_{20}\right) \right) + (1-\pi)\left(\left(\frac{1}{1-\pi}( - \hat{Y}_{21}\right) - \left(\frac{1}{1-\pi}(- \hat{Y}_{10}\right) \right)\right) + \left(\hat{Y}_{11} - \hat{Y}_{20}\right) + \left(\hat{Y}_{21} - \hat{Y}_{10}\right)$

$\frac12\left(Y_{11} - Y_{10} + Y_{21}- Y_{20}\right)$

@robins1994estimation

### Doubly robust estimation illustration


Consider this data (with confounding):

```{r}
# df with true treatment effect of 1 
# (0.5 if race = 0; 1.5 if race = 1)

df <- fabricatr::fabricate(
  N = 5000,
  class = sample(1:3, N, replace = TRUE),
  race = rbinom(N, 1, .5),
  Z = rbinom(N, 1, .2 + .3*race),
  Y = .5*Z + race*Z + class + rnorm(N),
  qsmk = factor(Z),
  class = factor(class),
  race = factor(race)
)
```

### Simple approaches

Naive regression produces biased estimates, even with controls. Lin regression gets the right result however.

```{r}
# Naive
lm_robust(Y ~ Z, data = df)$coefficients[["Z"]]

# OLS with controls
lm_robust(Y ~ Z + class + race, data = df)$coefficients[["Z"]]

# Lin
lm_lin(Y ~ Z,  ~ class + race, data = df)$coefficients[["Z"]]

```

### Doubly robust estimation

`drtmle` is an R package that uses doubly robust estimation to compute "marginal means of an outcome under fixed levels of a treatment."

```{r}
library(SuperLearner)
library(drtmle)
drtmle_fit <- drtmle(
  W = df |> select(race, class), 
  A = df$Z, 
  Y = df$Y, 
  SL_Q = c("SL.glm", "SL.mean", "SL.glm.interaction"),
  SL_g = c("SL.glm", "SL.mean", "SL.glm.interaction"),
  SL_Qr = "SL.glm",
  SL_gr = "SL.glm", 
  maxIter = 1
)
```

### Doubly robust estimation

```{r}
# "Marginal means"
drtmle_fit$drtmle$est

# Effects
ci(drtmle_fit, contrast = c(-1,1))
wald_test(drtmle_fit, contrast = c(-1,1))

```

Resource: <https://muse.jhu.edu/article/883477>

### Assessing performance

**Challenge**: Use `DeclareDesign` to compare performance of `drtmle` and `lm_lin`



## Principle: Keep the reporting close to the design

### Design-based analysis {.smaller}

Report the analysis that is implied by the design.

|              |              | T2                  |                     |                     |                                 |
|------------|------------|------------|------------|------------|------------|
|              |              | N                   | Y                   | All                 | Diff                            |
| T1           | N            | $\overline{y}_{00}$ | $\overline{y}_{01}$ | $\overline{y}_{0x}$ | $d_2|T1=0$                      |
|              |              | (sd)                | (sd)                | (sd)                | (sd)                            |
|              | Y            | $\overline{y}_{10}$ | $\overline{y}_{10}$ | $\overline{y}_{1x}$ | $d_2|T1=1$                      |
|              |              | (sd)                | (sd)                | (sd)                | (sd)                            |
|              | All          | $\overline{y}_{x0}$ | $\overline{y}_{x1}$ | $y$                 | $d_2$                           |
|              |              | (sd)                | (sd)                | (sd)                | (sd)                            |
|              | Diff         | $d_1|T2=0$          | $d_1|T2=1$          | $d_1$               | [$d_1d_2$]{style="color:green"} |
|              |              | (sd)                | (sd)                | (sd)                | (sd)                            |

This is instantly recognizable from the design and returns all the benefits of the factorial design including all main effects, conditional causal effects, interactions and summary outcomes. It is much clearer and more informative than a regression table.

