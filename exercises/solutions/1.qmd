---
format: 
   revealjs:
    embed-resources: true
    theme: serif
    slide-level: 3
    slide-number: true
    show-slide-number: all
    preview-links: auto
title: "Sample solutions"
author: "Macartan"
params:
  run: FALSE
execute: 
  echo: true
  warning: false
  error: false
  cache: false
---

# Set up

```{r}
library(tidyverse)
library(DeclareDesign)
library(knitr)
library(CausalQueries)
library(dagitty)
```

# 2.1 False positives {.smaller}

The exercise was:

 * Generate a simple experimental design from scratch in which we can vary the `N` and *in which there is no true effect* of some treatment.
 * Plot the distribution of $p$ values from the `simulations_df`. What shape is it and why?
 * Plot the power as $N$ increases, using the `diagnosands_df`
 * Plot the estimates against $p$ values for different values of $N$; what do you see?


## Answer: Design  {.smaller}

```{r, echo = TRUE}
N <- 100
design <- 
  declare_model(N = N, U = rnorm(N), potential_outcomes(Y ~  U)) + 
  declare_assignment(Z = simple_ra(N), Y = reveal_outcomes(Y ~ Z)) + 
  declare_inquiry(ate = mean(Y_Z_1 - Y_Z_0)) + 
  declare_estimator(Y ~ Z, inquiry = "ate", .method = lm_robust)
```

```{r, eval = FALSE}
diagnosis <- design |> redesign(N =  c(10, 1000, 1000, 10000)) |> 
  diagnose_design(sims = 2000)
```

```{r, echo = FALSE}
if(params$run)
  design |> redesign(N =  c(10, 1000, 1000, 10000)) |> 
  diagnose_design(sims = 2500) |>
  write_rds("saved/2.1.rds")
diagnosis <-   read_rds("saved/2.1.rds")
``` 

## results 1  {.smaller}

```{r, echo = TRUE}
diagnosis$simulations_df |> ggplot(aes(p.value)) + geom_histogram() + facet_grid(~N)
```

## results 2  {.smaller}

```{r, echo = TRUE}
diagnosis$diagnosands_df |> ggplot(aes(N, power)) + geom_line() + ylim(0, 1) +  scale_x_log10() 
```

## results 3  {.smaller}

```{r, echo = TRUE}

diagnosis$simulations_df |> ggplot(aes(estimate, p.value)) + geom_point() +   facet_grid(~N)
```

The distribution of $p$ values is uniform regardless of $N$. The power is flat, at 5%. However with larger N lower p values are achieved with lower estimates

# 2.2 Clustering  {.smaller}

Exercise:

* Say you assign a treatment at the classroom level. Should you cluster your standard errors at the level of the school or at the level of the classroom?
* Declare a design with this hierarchical data structure. Allow for the possibility that treatment effects vary at the school level. Assess the performance of the standard errors when you cluster at each of these levels (and when you do not cluster at all).
* Examine whether the performance depends on whether you are interested in the population average effects or the sample average effects.


## Design  {.smaller}

```{r, echo = TRUE}
design <- 
  declare_model(
    school = add_level(N = 20, u = rnorm(N, 0, 5)),
    class = add_level(N = 5, v = rnorm(N)),
    student =add_level(N = 5, w = rnorm(N))
    ) +
  declare_model(potential_outcomes(Y ~  Z + Z*u + v + w)) + 
  declare_assignment(Z = cluster_ra(clusters = class), Y = reveal_outcomes(Y ~ Z)) +
  declare_inquiry(PATE = 1,
          SATE = mean(Y_Z_1 - Y_Z_0)) +
  declare_estimator(Y~Z, label = "a. none") +
  declare_estimator(Y~Z, clusters = class, label = "b. class") +
  declare_estimator(Y~Z, clusters = school, label = "c. school") 
```

## Diagnosis  {.smaller}


```{r, eval = FALSE, echo = TRUE}  
diagnosis <- diagnose_design(design, sims = 2000)
```

```{r, echo = FALSE}
if(params$run) 
diagnose_design(design, sims = 2500) |> write_rds("saved/2.2.rds")
diagnosis <- read_rds("saved/2.2.rds")
```

```{r}
diagnosis$diagnosands_df |> 
  ggplot(aes(estimator, coverage, color=inquiry)) + geom_point()

```


# 2.3 Standard errors (covariance of potential outcomes)  {.smaller}


## Puzzle:

* Generate a simple experimental design in which there is a correlation (`rho`) between the two potential outcomes (`Y_Z_0` and `Y_Z_1`). 
* Show the distribution of the estimates over different values of `rho`
* Assess the performance of the estimates of the standard errors and the coverage as `rho` goes from -1 to 0 to 1. Describe how coverage changes. (Be sure to be clear on what coverage is!)

## Design  {.smaller}

```{r}

rho = 0

design <- 
  declare_model(N = 1000,
                Y_Z_0 = rnorm(N),
                Y_Z_1 = 1 + correlate(rnorm, given = Y_Z_0, rho = rho)) +
  declare_inquiry(ate = mean(Y_Z_1 - Y_Z_0)) +
  declare_assignment(Z = simple_ra(N), Y = reveal_outcomes(Y ~ Z)) + 
  declare_estimator(Y ~ Z, inquiry = "ate", .method = lm_robust)

```

## Redesign and diagnosis  {.smaller}

```{r, eval = FALSE}
diagnosis <-
  design |> redesign(rho = seq(-1, 1, length = 5)) |> 
  diagnose_design()
```

## How the sampling distribution depends upon `rho`  {.smaller}

```{r, eval = params$run, echo = FALSE}
design |> redesign(rho = seq(-1, 1, length = 5)) |> 
  diagnose_design(sims = 2000) |>
  write_rds("saved/2.3.rds")
```

```{r, echo = FALSE}
diagnosis <- read_rds("saved/2.3.rds")

```

```{r}
diagnosis$simulations_df |> ggplot(aes(estimate)) +
  geom_histogram() + facet_grid(~rho)

```

## How coverage depends upon `rho`  {.smaller}


```{r}
diagnosis$diagnosands_df |> ggplot(aes(rho, coverage)) + 
  geom_line() +
  geom_hline(yintercept = .95, color = "red")

```

# 2.4 Confounded  {.smaller}

## Puzzle   {.smaller}

* The assignment of a  treatment $X$ depends in part on upon some other, binary, variable $W$: in particular  $\Pr(X=1|W=0) = .2$ and $\Pr(X=1|W=1) = .5$)
* The outcome $Y$ depends on both $X$ and $W$: in particular $Y = X*W + u$ where $u$ is a random shock.
* Diagnose a design with three approaches to estimating the effect of $X$ on $Y$: (a) ignoring $W$ (b) adding $W$ as a linear control (c) including both $W$  and an interaction between $W$ and $X$. 

## Design  {.smaller}

```{r}

design <- 
  declare_model(N = 1000, 
                 W = rbinom(N, 1, .5),
                 U = rnorm(N, 0, .1),
                 X = rbinom(N, 1, .2 + .3*W),
                 Y = X*W + U) +
  declare_inquiry(ATE = mean(W)) +
  declare_estimator(Y ~ X, label = "naive") + 
  declare_estimator(Y ~ X + W, label = "control") + 
  declare_measurement(W_demeaned = W - mean(W)) +
  declare_estimator(Y ~ X*W_demeaned, label = "interacted")  

```

## Diagnosis  {.smaller}

```{r, eval = FALSE}
diagnose_design(design)
```

```{r, eval = params$run, echo = FALSE}
diagnose_design(design) |> write_rds("saved/2.4.rds")
```


```{r, echo = FALSE}

read_rds("saved/2.4.rds") |> reshape_diagnosis() |> select(Estimator, 'Mean Estimand', 'Mean Estimate', Bias, Coverage) |>  kable()

```

## Notes on 2.4  {.smaller}

The estimate from the naive model is wildly off because it effectively weights the strata with W=1 (where there is an effect of 1) according to its variance in treatment assignment (.5*.5) and the W=0 strata according to its variance (.2 * .8), giving an expected estimate of .25/(.25 + .16) = .61.

The '"saturated" regression deals with this, but to get the average estimate you have to average the estimate in the W=0 and in the W=1 condition.  There are different ways to do this but here we do it by demeaning W, then the coefficient on the "main" term actually captures the average.

# 3.1 potential outcomes  {.smaller}

## Puzzle   {.smaller}

$Y$  can be affected by two variables $X_1$ and $X_2$. All variables  binary. Fill in the potential outcomes (rows) for each of the column types:

|          | X1 is a necessary and sufficient condition for Y | X1 is necessary but not sufficient | X1 is sufficient but not necessary | X1 sometimes causes Y but is neither necessary nor sufficient |
|-------------------|--------------------------------------------------|-------------------------------------|-------------------------------------|----------------------------------------------------------------|
| Y(0,0) = |        0                           |   0         |     0     |      0    |
| Y(1,0) = |        1                           |   0         |     1     |      1    |
| Y(0,1) = |        0                           |   0         |     1     |      1    |
| Y(1,1) = |        1                           |   1         |     1     |      0    |


# 3.2 Potential outcomes two path model   {.smaller}

Consider an outcome Y that can be affected by two variables X1 and X2 but say that X2 can itself be affected by X1.  Write down possible potential outcomes for Y1 and X2 when: 	X1 causes X2 and Y, but X1 does not cause Y through X2

|Potential outcome |  Value (0/1)      |	
|---------|---------|
|Y(0,0) = | 0       |	
|Y(1,0) =	| 1 |
|Y(0,1) =	| 0 |
|Y(1,1) =	| 1 |
|X2(0) =	|0|
|X2(1) =	|1|

In this example X2 is irrelevant for Y.

# 3.2 Potential outcomes two path model   {.smaller}

## Puzzle   {.smaller}

Consider an outcome Y that can be affected by two variables X1 and X2 but say that X2 can itself be affected by X1.  Write down possible potential outcomes for Y1 and X2 when: 	X1 causes X2 and Y, but X1 does not cause Y through X2

## Possibilities   {.smaller}

|Potential outcome |  Value (0/1)      |	
|---------|---------|
|Y(0,0) = | 0       |	
|Y(1,0) =	| 0 |
|Y(0,1) =	| 0 |
|Y(1,1) =	| 1 |
|X2(0) =	|0|
|X2(1) =	|1|

Compare to this  example where X2 is relevant for Y. Here X1 has a direct  effect on Y that depends on the effect X1 has on X2. If the effect of X1 on X2 were blocked then X1 would not cause Y. If X2 were forced to X2 = 1 then X1 would have an effect despite not changing altering X2.




# 3.3 Rival causes  {.smaller}

## Puzzle   {.smaller}

Consider a process with: Y(0,0) =0,   Y(1,0) = 1, Y(0,1) =1, Y(1,1) =1.

* Say X1=1, X2=1. Then Y = 1. What caused Y = 1?

## Answer   {.smaller}

Answer: arguably neither since X1 makes no difference given the value of X2, and similarly   X2 makes no difference given the value of X1.

One could also argue that the package of X1 and X2 produced the effect, though this depends on a coarsening of the treatments.

# 3.4 Rival causes  {.smaller}

## Puzzle 1  {.smaller}

Consider a process with: Y(0,0) =0,   Y(1,0) = 1, Y(0,1) =1, Y(1,1) =1.

* Say X1=0, X2=0. Then Y = 0. What caused Y = 0?


## Answer   {.smaller}

Arguably both since X1 makes a difference given the value of X2, and similarly   X2 makes a difference given the value of X1.

# 3.4 Rival causes  {.smaller}

## Puzzle 2  {.smaller}

Consider a process with: Y(0,0) =0,   Y(1,0) = 1, Y(0,1) =1, Y(1,1) =1.

* Say X1 = 1 with 10% probability, otherwise 0 and, independently,  X2 = 1 with 50% probability, otherwise 0. 

Then what is the average effect of X1 on Y? What is the average effect of X2 on Y? Which cause has the biggest effect?

## Answer   {.smaller}

* X1 has an effect 50% of the time
* X2 has an effect 10% of the time

So the average effect of X1 on X2 is bigger. 

## Answer   {.smaller}

However: 

* whenever an outcome of 1 is observed the "magnitude" of the effect of the two causes is the same (1).
* whenever an outcome of 0 is observed there is a .45/.95 = .47 chance that X1 would have made a difference and a .05/.95 $\approx .05$ chance that X2 would have made a difference


|        | X2 = 0 | X2 = 1 |
|--------|--------|--------|
| X1 = 0 | 0.45   | 0.45   |
| X1 = 1 | 0.05   | 0.05   |

*The rare event is more likely to have been pivotal* when causes are complements

# 3.3 Effects on changes  {.smaller}

## Puzzle   {.smaller}

* A set of units have outcome $Y^1_i$ at baseline.
* At endline they have potential outcomes $Y^2_i(0)$ and  $Y^2_i(1)$


1. Write down the estimand for the average effect of treatment on endline outcomes
2. Write down the estimand for the average effect of treatment on the change from baseline to endline for all units

## Answer   {.smaller}

1.  $$\mathbb{E}[Y^2(1) - Y^2(0)]$$
2.  $$\mathbb{E}[(Y^2(1) - Y^1) - (Y^2(0) - Y^1)] = \mathbb{E}[Y^2(1) - Y^2(0)] - \mathbb[Y^1 - Y^1] = \mathbb{E}[Y^2(1) - Y^2(0)]$$

Note that since we are taking expectations over *all* units, the baseline differences drop out. The estimands are the same.


# 4.1 Colliders  {.smaller}


## Puzzle   {.smaller}

* Declare a simple design in which (i)  $X$ and $Y$ both have a positive effect on  (binary) $K$ but $X$ does not cause $Y$ (ii) a researcher conditions on $K==1$ when estimating the effect of $X$ on $Y$
* Show that this can generate biased results. Can you find situations where the bias can be either positive or negative?

## Declaration  {.smaller}

```{r}

design <- 
  
  declare_model(
    N = 10000,
    X = rbinom(N, 1, .5),
    Y = rbinom(N, 1, .5),
    K1 = rbinom(N, 1, .1 + .05*X + .05*Y + .7*X*Y),
    K2 = rbinom(N, 1, .1 + .2*X + .2*Y + -.2*X*Y)) +
  declare_inquiry(ATE = 0) +
  declare_estimator(Y ~ X,  label = "No conditioning")+
  declare_estimator(Y ~ X, subset = K1 == 1, label = "K1 conditioning")+
  declare_estimator(Y ~ X, subset = K2 == 1, label = "K2 conditioning")


```

## 4.1 Diagnosis  {.smaller}

```{r, eval = FALSE}

design |> diagnose_design()

```


```{r, eval = params$run}

design |>  diagnose_design() |> write_rds("saved/4.1.rds")

diagnosis <- read_rds("saved/4.1.rds") 

```

```{r}
diagnosis |>  reshape_diagnosis() |> select(Estimator, 'Mean Estimand', 'Mean Estimate', "Bias") |> kable()
```

Note depending on which set we condition on we can get positive or negative bias in estimates; what matters is the way that $X$ and $Y$ interact to produce the conditioning set. 


# 4.2  Backdoor {.smaller}

## Puzzle   {.smaller}

* Draw a DAG with 5 nodes representing  a situation in which $X$ causes $Y$ though $M$, $C$ affects both $X$ and $M$ and $D$ affects both $M$ and $Y$.
* Figure out a set of nodes which, when controlled for, allow for the identification of the effect of $X$ on $Y$
* Represent it in `dagitty` and check your answer
* Bonus: Declare the design and compare the behavior of designs that do and do not control for these nodes.

## Answer   {.smaller}

```{r}
statement <-"X -> M -> Y; X <- C -> M; M <- D -> Y"

statement |> make_model() |> plot()

g <- dagitty( "dag{X -> M -> Y; X <- C -> M; M <- D -> Y}" )
adjustmentSets( g, "X", "Y" ) 
```

# 4.3 Find a DAG {.smaller}

## Puzzle   {.smaller}

Make a DAG that is consistent with this distribution

```{r, echo = TRUE}
expand_grid(A=0:1, B = 0:1, C= 0:1)|> 
  mutate(p = c(.64, .16, .16, .04,  .16, .24, .24, .36)/2) |>
  kable()

```

Set up a model in `DeclareDesign` that has this distribution. Draw a large dataset from it and check if relations of conditional independence implied by your DAG.


## 4.3 Answer  {.smaller}

We have a table of probabilities for all data realizations for 3 binary nodes. This distribution is consistent with some but not all relations of conditional independence, which makes them consistent with some but not all DAGs.

We need to make sense of the conditional probabilities implied here.

* We see for example that $\Pr(C=1) = .25$ when $A=0$, independent of $B$ but $\Pr(C=1) = .6$ when $A=1$, independent of $B$.

So: *C and B are conditionally independent given A*

* The probability that $A=1$ however is different for all patterns of B and C

These are our clues.

## 4.3 Answer  {.smaller}

Calculations confirming these clues:

```{r, echo = TRUE}
df <- expand_grid(A=0:1, B = 0:1, C= 0:1)|> 
  mutate(p = c(.64, .16, .16, .04,  .16, .24, .24, .36)/2) 

df |> group_by(B, C) |> mutate(p = p/sum(p)) |> ungroup()  |> filter(A==1)
df |> group_by(A, B) |> mutate(p = p/sum(p)) |> ungroup()  |> filter(C==1)

```


## 4.3 Answer  {.smaller}

So one possibility is $B \rightarrow A \rightarrow C$

For this DAG the probability of the data is given by:

$$\Pr(b)\Pr(a|b)\Pr(c|a)$$

Now lets figure out these probabilities and conditional probabilities

## 4.3 Find a DAG, find probabilities  {.smaller}

The probability that $C=1$ is 0.4:

```{r}
df |> filter(C==1) |> summarize(sum(p))
```

The probability that $A=1$ given $C=1$: 0.75

```{r}
df |> filter(C==1) |> mutate(p = p/sum(p)) |> filter(A==1) |> summarize(sum(p))
```

The probability that $A=1$ given $C=0$: 0.75

```{r}
df |> filter(C==0) |> mutate(p = p/sum(p)) |> filter(A==1) |> summarize(sum(p))
```


## 4.3 Find a DAG, find probabilities  {.smaller}

The probability that $B=1$ given $A=1$: 0.75

```{r}
df |> filter(A==1) |> mutate(p = p/sum(p)) |> filter(B==1) |> summarize(sum(p))
```

The probability that $B=1$ given $A=0$: 0.2

```{r}
df |> filter(A==0) |> mutate(p = p/sum(p)) |> filter(B==1) |> summarize(sum(p))
```

## 4.3 Find a DAG, find probabilities  {.smaller}

So for example:

* $\Pr(A = 1 \& B = 1 \& C = 1) =  .4*.6*.75 = 0.18$

and so on

## 4.3 Design declaration


We will just do the model step:

```{r}

model <- declare_model(N = 10000,
                    C = rbinom(N, 1,  .4),
                    A = rbinom(N, 1, prob = ifelse(C==1, .75, 1/3)),
                    B = rbinom(N, 1, prob = ifelse(A==1, .6, .2)))

model() |> group_by(A, B, C) |> summarize(p = n()/10000) |> round(2)
```



# 4.4 Front door criterion {.smaller}

## 4.4 Answer  {.smaller}

The first example satisfies the "front door criterion" which we cover in week 2. 

The second  example is interesting because it seems to also satisfy the "front door criterion" but now all data events  do not have positive probability. For example, we would never see $X=1, M=0, Y=1$

In fact the dats we do see, with $X$, $M$ and $Y$ all perfectly correlated are consistent with different possible data generating processes, for instance:

* Unobserved $U$ causes both $X$ and $Y$ with 100% effect
* $X$ causes $M$ with 100% effect
* $M$ has no impact on $Y$

Because this is consistent with the data the alternative model with $X$ causing $M$ and $M$ causing $Y$ is not identified


## 4.4 Front door criterion with `CausalQueries`  {.smaller}

We'll first use a function that makes data consistent with an $X \rightarrow M \rightarrow Y$ model  with no confounding and overall causal effect of $b$. Note that although there is no actual confounding in the stipulated data generating process, this is not known to the researcher. 

```{r, echo = TRUE}
library(fabricatr)

# A function to generate data consistent with an effect of b
data_function = function(b)
  fabricate(N = 10000, 
            X = rep(0:1, N/2), 
            M = rbinom(N, 1, (1-b^.5)/2  + b^.5*X),
            Y = rbinom(N, 1, (1-b^.5)/2  + b^.5*M))

```

## Updating Code {.smaller}

```{r, eval = FALSE, echo = TRUE}
library(CausalQueries)
options(mc.cores = parallel::detectCores())

bs <- c(.5, .75, .85, .9, .95, 1)

updated_models <- bs |> lapply(function(b)  
  model |> update_model(data_function(b))
  )

updated_models |>
  query_model("Y[X=1] - Y[X=0]", using = "posteriors") |>
  ggplot(aes(bs, mean)) + geom_point() + theme_bw() + 
  xlab("true effect") + ylab("posterior mean")
```

## Updating Results  {.smaller}

```{r, echo = FALSE}
library(CausalQueries)
options(mc.cores = parallel::detectCores())

bs <- c(.5, .75, .85, .9, .95, 1)


if(params$run)
  bs |> lapply(function(b)  
  model |> update_model(data_function(b), iter = 5000)
  ) |>
  write_rds("saved/frontdoorCQ.rds") 


read_rds("saved/frontdoorCQ.rds") |>
  query_model("Y[X=1] - Y[X=0]", using = "posteriors") |>
  ggplot(aes(bs, mean)) + geom_abline(color = "orange")  + 
  geom_point() + geom_errorbar(aes(ymin = cred.low, ymax = cred.high)) + 
  xlab("true effect") + ylab("posterior mean")   + theme_bw() 
```

# 5.1 ATE, ATC, ATT  {.smaller}

## Puzzle 1 {.smaller}

| Block | Z | Y |
|-------|---|---|
| 1     | 0 | 0 |
| 1     | 0 | 0 |
| 1     | 1 | 1 |
| 2     | 0 | 0 |
| 2     | 0 | 0 |
| 2     | 0 | 1 |
| 2     | 1 | 0 |
| 2     | 1 | 1 |
| 2     | 1 | 1 |

* Can you estimate the ATE? How about the ATT? And the ATC?
* How do these compare to a simple difference in means between treatment and control? 

## Answer 1

* In Block 1 the estimated effect is $1 - 0 = 1$
* In Block 2 the estimated effect is $\frac23 - \frac 13 = \frac13$ 
* The ATE estimate is then $\frac39 \times 1 + \frac69 \times \frac 13 =  \frac{5}9$
* The ATC estimates is  $\frac25 \times 1 + \frac 35 \times \frac 13 =  \frac{13}{25}$
* The ATT estimates is  $\frac14 \times 1 + \frac 34 \times \frac 13 =  \frac{6}{12}$
* Simple DIM is $\frac{3}4 - \frac15 = \frac{11}{20}$

These are not massively different in this example

## Declaration {.smaller}

* Use `DeclareDesign` to compare the answers you get if you use (1) OLS and controlled for block (2)  IPW and (3)  the Lin estimator

```{r}

df <- data.frame(Block = c(1,1,1,2,2,2,2,2,2),
                   Z = c(0,0,1,0,0,0,1,1,1),
                   Y = c(0,0,1,0,0,1,0,1,1)) |>
  group_by(Block) |> 
  mutate(p = mean(Z), ipw = Z/p + (1-Z)/(1-p)) |>
  ungroup() |> data.frame()

design <- declare_model(data = df) +
  declare_inquiry(ATE = 5/9) +
  declare_estimator(Y ~ Z + Block, label = "FE") +
  declare_estimator(Y ~ Z, weights = ipw, label = "IPW") +
  declare_estimator(Y ~ Z, covariates = ~ Block, .method = lm_lin, label = "lin") 

```

## Diagnosis {.smaller}


```{r, message = FALSE, warning = FALSE}

diagnosands <- declare_diagnosands(
                  bias = mean(estimate - estimand),
                  sd_estimate = sd(estimate),
                  mean_se = mean(std.error))

diagnose_design(design, diagnosands = diagnosands, sims = 2)

```



## Diagnosis {.smaller}

* Let the size of the data increase by  factor $k$. Show how  precision changes


```{r, message = FALSE, warning = FALSE}
replicate <- function(df, w) df |> uncount(w) # uncount used here to replicate data

design |> 
  insert_step(after = 1, declare_measurement(handler = replicate, w = k)) |>
  redesign(k = 1:5) |> 
  diagnose_design(diagnosands = diagnosands, sims = 2) |> tidy() |>
  ggplot(aes(k, estimate, color = estimator)) + geom_line() + facet_grid(~diagnosand, scales = "free")
  

```

## Diagnosis {.smaller}

The OLS bias never goes away. The precision gains from lin over IPW gradually drop out.
