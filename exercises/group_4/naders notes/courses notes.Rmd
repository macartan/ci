---
title: "Naders Course Notes"
output: html_notebook
---

```{r warning=FALSE, echo=FALSE, message=FALSE}
pacman::p_load(tidyverse,
               DeclareDesign,
               CausalQueries,
               fabricatr,
               gridExtra
)
```

# Session 2

## Lecture (Causality. What's a cause?)
### Potential outcomes and the counterfactual approach

**Causation as difference making**

+ Intervention approach: Intervention changes Outcome
+ Need to know
  - What happened
  - What would the outcome have been without intervention
  - ==> Fundamental Problem of Causal Inference

### Potential Outcomes

+ Post treatment outcomes (counterfactual)
  + $Y_i(1)$ received treatment
  + $Y_i(0)$ did not
  + Causal effect: $\tau_i = Y_i(1)-Y_i(0)$
  + Causal effect of a treatment defined at the individual level 
  + There is no "data generating process" or functional form (just two quantities, no covariates etc.)
  
"Did Germany cause the second world war?" Compared to what? Germany not exist or specific action? Ask about specific conditions?

Are there any substantive assumptions made here so far? No!

#### Transivity and Connectedness

Now thatwe have a concept of causal effects available, let's answer two questions:

+ TRANSIVITY: If for a given unit A causes B and B causes C, does that mean A causes C?
+ A boulder is flying down a mountain. You duck. This saves your life.
+ So the boulder caused the ducking and the ducking caused you to survive
+ So: did the boulder cause you to survice? No but the join event of A and B

---

Connectedness: Say A causes B - does that mean that there is a spatiotemporally
continuous sequence of causal intermediates?

> Person A is planning some action Y; Person B sets out to stop them; person X intervenes and prevents person B from stopping person A. In this case Person A may complete their action, producing Y, without any knowledge that B and X even exist; in particular B and X need not be anywhere close to the action. So: did X cause Y?

#### Causal claims: Contribution or attribution

Consider an outcome $Y$ that might depend on two causes $X_1$ and $X_2$

$Y(0,0)=0$
$Y(1,0)=0$
$Y(0,1)=0$
$Y(1,1)=1$

What caused $Y$? Which cause was most important?

**Answer:** Both to both. In this model no further probabilistic statements can be made.
We can not say more or less here!

**Example:** Erdogan’s increasing authoritarianism was the most important reason for the attempted coup:

  + More important than Turkey’s history of coups? (A: They are not rivaled and could be both 100%)
  + What does that mean?

#### No causation without manipulation

+ What does that mean for immutable characteristics?
+ But we can work with perception but otherwise we can't...

Physicists objection: What does it mean to say that the tides are caused by the moon? What exactly do we have to imagine…?

#### Causal claims: What is actually seen?

-   We have talked about what's potential, now what do we *observe*?
-   Say $Z_i$ indicates whether the unit $i$ is assigned to treatment $(Z_i=1)$ or not $(Z_i=0)$. It describes the treatment process. Then what we observe is: $$ Y_i = Z_iY_i(1) + (1-Z_i)Y_i(0) $$

This is sometimes called a "switching equation"

In `DeclareDesign` $Y$ is realised from potential outcomes and assignment in this way using `reveal_outcomes`

---

-   Say $Z$ is a random variable, then this is a sort of data generating process. BUT the key thing to note is
-   $Y_i$ is random but the randomness comes from $Z_i$ --- the potential outcomes, $Y_i(1)$, $Y_i(0)$ are fixed
-   Compare this to a regression approach in which $Y$ is random but the $X$'s are fixed. eg: $$ Y \sim N(\beta X, \sigma^2) \text{ or }  Y=\alpha+\beta X+\epsilon, \epsilon\sim N(0, \sigma^2) $$
    
==> Beta is kinda like Delta, it's the difference

---

You can never measure the effect of something you can INFERE

-   The causal effect of Treatment (relative to Control) is: $$\tau_i = Y_i(1) - Y_i(0)$$
-   This is what we want to estimate.
-   BUT: We never can observe both $Y_i(1)$ and $Y_i(0)$!
-   This is the **fundamental problem** (@holland1986statistics)

Even if you do it as a lab thing you kinda thing you still have a time assumption

---
**Solution:**

-   Now for some magic. We really want to estimate: $$ \tau_i = Y_i(1) - Y_i(0)$$

-   BUT: We never can observe both $Y_i(1)$ and $Y_i(0)$

-   Say we lower our sights and try to estimate an *average* treatment effect: $$ \tau = \mathbb{E} [Y(1)-Y(0)]$$

-   Now make use of the fact that $$\mathbb E[Y(1)-Y(0)]  = \mathbb E[Y(1)]- \mathbb E [Y(0)] $$

-   In words: *The average of differences is equal to the difference of averages*; here, the average treatment effect is equal to the difference in average outcomes in treatment and control units.

-   The magic is that *while we can't hope to measure the differences; we are good at measuring averages*.

==> The average difference is the difference of averages of this and that
==> We cannot observe it on the invidiual unit but averaging we can


-   So we want to estimate $\mathbb{E} [Y(1)]$ and $\mathbb{E} [Y(0)]$.
-   We know that we can estimate averages of a quantity by taking the average value from a random sample of units
-   To do this here we need to select a random sample of the $Y(1)$ values and a random sample of the $Y(0)$ values, in other words, we **randomly assign** subjects to treatment and control conditions.
-   When we do that we can in fact estimate: $$ \mathbb {E}_N[Y_i(1) | Z_i = 1) - \mathbb {E}_N(Y_i(0) | Z_i = 0]$$ which in expectation equals: $$ \mathbb{E} [Y_i(1) | Z_i = 1 \text{ or } Z_i = 0] - \mathbb{E} [Y_i(0) | Z_i = 1 \text{ or } Z_i = 0]$$
-   This highlights a deep connection between **random assignment** and **random sampling**: when we do random assignment *we are in fact randomly sampling from different possible worlds*.

Survey = Sample = Weight = Experiment = Propensity Matching

Selbst bei random assignment müsste man darauf eingehen ob es biases gibt (aber müssen diese characteristics auch nicht korrelieren mit dem outcome?)


This provides a **positive argument** for causal inference from randomization, rather than simply saying with randomization "everything else is controlled for"

**Let's discuss:**

-   *Does the fact that an estimate is unbiased mean that it is right?* NO!
-   *Can a randomization "fail"?* Depends theoratically no but practically people argue (not perfect balance?)
-   *Where are the covariates* Are in the i substript

#### Radnomization works

**Idea**: random assignment is random sampling from potential worlds: to understand anything you find, you need to know the sampling weights

```{r,eval=TRUE, echo = FALSE}
po.graph <- function(N, Y0, Y1, u, Z, yl = "Y(0) & Y(1)") {
  # Combine data into a data frame
  data <- data.frame(u = u, Y0 = Y0, Y1 = Y1, Z = Z)
  
  # Plot 1
  p1 <- 
    ggplot(data, aes(x = u, y = Y0)) +
    geom_point(col = "#619CFF", alpha = 0.7) +
    geom_point(aes(y = Y1), col = "#F8766D", alpha = 0.5) +
    ylim(-3, 4) +
    xlim(1, N) +
    labs(x = "u", y = yl) +
    ggtitle("Y(1) and Y(0) for all units")   + theme_bw()
  
  # Plot 2
  p2 <- ggplot(data, aes(x = u, y = Y1 - Y0)) +
    geom_point(aes(y = Y1 - Y0),  alpha = 0.7) +
    geom_segment(aes(x = u, xend = u, y = 0, yend = Y1 - Y0), 
                 alpha = 0.5) +
    ylim(-3, 4) +
    xlim(1, N) +
    labs(x = "u", y = yl) +
    ggtitle("Y(1) - Y(0)") +
    theme(legend.position = "none") + theme_bw()
  
  # Plot 3
  p3 <- ggplot(data, aes(x = u, y = ifelse(Z == 0, Y0, Y1))) +
    geom_point(col = "#619CFF", alpha = 0.7) +
    geom_point(aes(y = Y1), col = "#F8766D", alpha = 0.5) +
    geom_hline(yintercept = mean(data$Y0[Z == 0]), col = "black") +
    geom_hline(yintercept = mean(data$Y1[Z == 1]), col = "#F8766D") +
    ylim(-3, 4) +
    xlim(1, N) +
    labs(x = "u", y = yl) +
    ggtitle("Y(1| Z=1) and Y(0| Z=0)") +     theme_bw() +
    theme(legend.position = "none")  
  
  # Plot 4
  p4 <- ggplot(data, aes(x = u, y = Y0)) +
    geom_point(col = "#619CFF", alpha = 0.7) +
    geom_point(aes(y = Y1), col = "#F8766D", alpha = 0.5) +
    geom_segment(data = data[Z == 0 & u <= N/2, ], 
                 aes(x = 1, xend = N/2, y = mean(Y0[Z == 0 & u <= N/2]), yend = mean(Y0[Z == 0 & u <= N/2])), lwd = 1.3, col = "#619CFF") +
    geom_segment(data = data[Z == 1 & u <= N/2, ], 
                 aes(x = 1, xend = N/2, y = mean(Y1[Z == 1 & u <= N/2]), yend = mean(Y1[Z == 1 & u <= N/2])), lwd = 1.3, col = "#F8766D") +
    geom_segment(data = data[Z == 0 & u > N/2, ], 
                 aes(x = N/2, xend = N, y = mean(Y0[Z == 0 & u > N/2]), yend = mean(Y0[Z == 0 & u > N/2])), lwd = 1.3, col = "#619CFF") +
    geom_segment(data = data[Z == 1 & u > N/2, ], 
                 aes(x = N/2, xend = N, y = mean(Y1[Z == 1 & u > N/2]), yend = mean(Y1[Z == 1 & u > N/2])), lwd = 1.3, col = "#F8766D") +
    
    ylim(-3, 4) +
    xlim(1, N) +
    labs(x = "u", y = yl) +
    ggtitle("Subgroup ATEs") + 
    theme_bw() +
    theme(legend.position = "none")
  
  # Display all plots together
  gridExtra::grid.arrange(p1, p2, p3, p4, ncol = 2)
}

N  <- 100
u  <- seq(1:N)
Y0 <- rnorm(N)
Y1 <- rnorm(N) + 1
Z  <- 1:N %in% sample(N, N/2)
```

```{r, echo = FALSE}
set.seed(1)
```

```{r,eval=TRUE, echo = FALSE, fig.width = 14, fig.height= 7, warning = FALSE}
po.graph(N = 20 , Y0, Y1, u, Z)	
```

## Exercises 2 & 3
### 2.1 Power will varry on size / 3.1

==> ANtworten
==> Larger the effect the better the p-Value
==> But the larger the N the less larger effects
==> So p-value consistent

### 2.2./3.2.

CLUSTERN  on treatment level wenn man aber sample shit machen möchte
aber möchte population averages usw raus holen machen möchte dann im höheren level
auch clustern?

Type a-d causation types mal angucken


### 2.3/3.3

## DAG

## Inquiries

+ inquiry your question
+ estimand = answer


