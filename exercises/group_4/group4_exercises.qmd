---
title: "Group 4 - Week 1 & 2"
subtitle: "Causal Inference and Experimental Design"
date: "19 January, 2024"
date-format: "long"
author:
  - name: Philipp Heyna
    email: "<a href='mailto:philipp.heyna@hu-berlin.de'><span>&#64;</span>philipp.heyna</a>"
  - name: Nader Hotait
    email: "<a href='mailto:nader.hotait@hu-berlin.de'><span>&#64;</span>nader.hotait</a>"
  - name: Daniel Kuhlen
    email: "<a href='mailto:kuhlenda@hu-berlin.de'><span>&#64;</span>kuhlenda</a>"
  - name: Yoanna Yankova
    email: "<a href='mailto:yankovay@hu-berlin.de'><span>&#64;</span>yankovay</a>"
format:
  revealjs: 
    theme: white
    logo: hu_siegel-kombi_rgb.png
    css: logo.css
    mbed-resources: true
    slide-level: 3
    slide-number: true
    show-slide-number: all
    preview-links: auto
    number-sections: false
    scrollable: true
    chalkboard: true
---

```{r packages, echo=FALSE}
# packages
pacman::p_load(tidyverse,
               DeclareDesign,
               CausalQueries,
               knitr,
               kableExtra)
```

## Week 1

## Q 2.4 (Confounded) {.smaller}

Declare a design in which:

* The assignment of a  treatment $X$ depends in part on upon some other, binary, variable $W$: in particular  $\Pr(X=1|W=0) = .2$ and $\Pr(X=1|W=1) = .5$)
* The outcome $Y$ depends on both $X$ and $W$: in particular $Y = X*W + u$ where $u$ is a random shock.
* Diagnose a design with three approaches to estimating the effect of $X$ on $Y$: (a) ignoring $W$ (b) adding $W$ as a linear control (c) including both $W$  and an interaction between $W$ and $X$. 

Discuss results. Do any of these return the right answer?

**Hint:** You can add three separate `declare_estimator` steps. They should have distinct labels. The trickiest part is to figure out how to extract the estimate in (c) because you will have both a main term and an interaction term for $X$.

## Model Declaration

```{r model_declaration_1, echo=TRUE}
#| code-line-numbers: "|3-6|7|8-10|11-16"
set.seed(123)
declaration <- 
  declare_model(
    N = 100, 
    U = rnorm(N),
    W = rbinom(N, 1, 0.5), +
    potential_outcomes(Y ~ X * W + U, conditions = list(X = 0:1))) +
  declare_inquiry(ATE = (mean(Y_X_1 - Y_X_0))) + 
  declare_assignment(X = rbinom(N, 1, prob = ifelse(W == 1, 0.5, 0.2))) + 
  declare_measurement(Y = reveal_outcomes(Y ~ X)) +
  declare_estimator(
    Y ~ X, term = "X", inquiry = "ATE", label = "a) Ignore W") +
  declare_estimator(
    Y ~ X + W, term = "X", inquiry = "ATE",label = "b) Control for W") +
  declare_estimator(
    Y ~ X + X * W + W, term = "X", inquiry = "ATE",label = "c) Interaction")
```

## Model Diagnosis

```{r echo=TRUE}
model_diagnosis <-
  diagnose_design(declaration,
                  sims = 500,
                  bootstrap_sims = 100)

reshape_diagnosis(model_diagnosis) %>%
  kable() %>%
  kableExtra::kable_styling(font_size = 18)
```

## Discussion {.smaller}

The role of $W$ is defined as a confounder since it serves as a parent variable influencing both the treatment assignment $X$ and the treatment outcome $Y$. Failing to adjust for this confounder in the model can lead to biased estimations. Such a bias is conventionally referred to as "common cause confounding bias" or, in other words, a violation of the backdoor criterion. Therefore, it is crucial to incorporate $W$ into the model. Utilizing simulations through `DeclareDesign`, we gain insights into this specific phenomenon. From the simulation results, it is evident that the model which accounts for $W$ (*"Control for W"*) produces mean estimates that are considerably closer and less biased compared to the estimand than the model that neglects $W$ (*"Ignore W"*). Furthermore, both the SD and RMSE metrics suggest that the estimates from the model accounting for the confounder are less erroneous compared to those from the model that disregards it.


## Q 3.4

+ A set of units have outcome $Y^1_i$ at baseline.
+ At endline they have potential outcomes $Y^2_i(0)$ and  $Y^2_i(1)$

1. Write down the estimand for the average effect of treatment on endline outcomes
2. Write down the estimand for the average effect of treatment on the change from baseline to endline for all units

Compare these and discuss.

## Discussion{.smaller}

Assuming:

1. $\text{ATE}_{\text{endline}} = \frac{1}{N} \sum_{i=1}^{N} \left( Y^2_i(1) - Y^2_i(0) \right)$
2. $\text{ATE}_{\text{change}} = \frac{1}{N} \sum_{i=1}^{N} \left( \left( Y^2_i(1) - Y^1_i \right) - \left( Y^2_i(0) - Y^1_i \right) \right)$

.   .   .   

One can conclude that $\text{ATE}_{\text{endline}}$ approximates an estimator, which informs us about the average size of the treatment effect on a binary outcome. However, $\text{ATE}_{\text{change}}$ provides insight into the treatment effect as a comparison between conditions before and after treatment, indicating the effectiveness of the treatment relative to not being treated. Hence, the first scenario informs us about the strength of the treatment given the prevalence of positive outcomes, while the second scenario reveals the effectiveness of the treatment by considering the initial conditions of the treated units.

.   .   .   

Assuming that $ATE = ATT$ and considering  $Y^{t=1}_i$ and $Y^{t=2}_i$ in this scenario, one could describe $\text{ATE}_{\text{endline}}$ as a between-estimator of treated units, and $\text{ATE}_{\text{change}}$ as an in-between estimation of treated units before and after treatment, effectively combining between- and within-unit estimations.

## Q 4.4 {.smaller}
Imagine a model that looks like this:

```{r, echo = FALSE, fig.height = 3, fig.width = 10}
make_model("X -> M -> Y <-> X") |> plot(x_coord = 1:3, y_coord = c(1,1,1))
```

* Say that in truth  ATE of  $X$ on $M$ is .9 and that the ATE of  $M$ on $Y$ is .9. Is the implied effect of 0.81 on $X$ on $Y$ identified?   
* Say that in truth  ATE of  $X$ on $M$ is 1 and that the ATE of  $M$ on $Y$ is 1. Is the implied effect of 1 on $X$ on $Y$ identified?   
* Discuss

**Hint**: This question is asking about the front door criterion. Check whether the conditions apply for the front door criterion on hold. Note that an effect is *not* identified if the data pattern it produces is also consistent with a different effect. Is that the case here? Note you can generate and update models of this form with `CausalQueries`.


## Discussion 
Three conditions have to be met for the front door criterion to hold:

+ $M$ completely mediates the effect of $X$ on Y, i.e., all causal paths from $X$ to $Y$ go through $M$
+ There us no unblocked backdoor path from $X$ to $M$
+ All backdoor paths from $M$ to $Y$ are blocked by $X$

## Scenario 1{.smaller}
**Direct causal path between both X and Y**

```{r fig.height = 3, fig.width = 10}
make_model("X -> M -> Y <-> X") %>%
  plot(x_coord = 1:3, y_coord = c(1,1,1))
```

The second and third assumption seem to hold for the model; there is only one path from X to M and the backdoor path from M to Y is blocked by X (thus controlled for). However, there is a second direct pathway from X to Y, meaning that the first condition is violated. 

With correlations of 0.9 between both X and M and M and Y, I would therefore argue that the causal effect cannot be identified. With perfect deterministic relationships between the variables (correlation = 1), though, I think that there cannot be a backdoor path between X and Y as there is no more variance that is not explained via the pathway through M.


## Scenario 2{.smaller}
**Confounder causing both X and Y**

```{r fig.height = 3, fig.width = 10}
make_model("X -> M -> Y <- C -> X") %>%
  plot(x_coord = c(2,1,2,3), y_coord = c(2,1,1,1))
```

Here, the front door criterion holds: The first condition is fulfilled because there is no causal path from X to Y other than the one through M.

## Using `CausalQueries` 1 {.smaller}

```{r echo=TRUE}
model <- make_model("X -> M -> Y <-> X")
model <- set_priors(model, distribution = "jeffreys")
data_1 <- fabricate(N = 6500, 
                  X = rep(0:1, N/2), 
                  M = rbinom(N, 1, 0.05 + .9*X), 
                  Y = rbinom(N, 1, 0.05 + .9*M))

model_1 <- model %>%
  update_model(data_1, refresh = 0)

query_model(
    model = model_1, 
    using = c("priors", "posteriors"),
    query = "Y[X=1] - Y[X=0]",
    )  %>%
  kable(digits = 2) %>%
  kableExtra::kable_styling(font_size = 18)
```


## Using `CausalQueries` 2 {.smaller}

```{r echo=TRUE}
data_2 <- fabricate(N = 6500, 
                  X = rep(0:1, N/2), 
                  M = X, 
                  Y = M)

model_2 <- model %>%
  update_model(data_2, refresh = 0)

query_model(
    model = model_2, 
    using = c("priors", "posteriors"),
    query = "Y[X=1] - Y[X=0]",
    )  %>%
  kable(digits = 2) %>%
  kableExtra::kable_styling(font_size = 18)
```

## Week 2

## Q 5.4

Say you want to include a control variable. But you have missingness in the control. Should you proceed and what can you do about it? Declare a design for an experiment in which a binary covariate X is related to potential outcomes, according to b, and so to treatment effects. Say X is missing with probability p.

Compare answer strategies in which you:

-   do not control for
-   do control for but drop whenever is missing and
-   Treat as a block in your analysis design with three values (0, 1, and missing).

### Varying N

```{r echo=TRUE}
p <- 0.5

# Model with standard random assignment
model_random <-
  declare_model(
    N = 100,
    b = 0.5,
    X = rbinom(N, size = 1, prob = 0.5),
    X_mi = ifelse(X == 1, # New version of X with missings (missing completely at random)
                  sample(x = c(1, NA), size = N, prob = c(1-p, p), replace = TRUE),
                  sample(x = c(0, NA), size = N, prob = c(1-p, p), replace = TRUE)),
    U = b * X + rnorm(N, mean = 0, sd = 1),
    potential_outcomes(Y ~ Z + b * X + U, conditions = list(Z = c(0, 1)))
  )

inquiry_ate <- 
  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0))

data_strategy_ra <- 
  declare_assignment(Z = complete_ra(N)) +
  declare_measurement(Y = reveal_outcomes(Y ~ Z))
  
answer_strategy_ra <-  
  declare_estimator(Y ~ Z, covariates = ~X, .method = lm_lin, inquiry = "ATE", label = "X Complete") +
  declare_estimator(Y ~ Z, inquiry = "ATE", label = "X Drop") +
  declare_estimator(Y ~ Z, covariates = ~X_mi, .method = lm_lin, inquiry = "ATE", label = "X Incomplete")

design_54_N_ra <- model_random + inquiry_ate + data_strategy_ra + answer_strategy_ra

# Model with block random assignment
model_blocks <-
  declare_model(
    N = 100,
    b = 0.5,
    X = rbinom(N, size = 1, prob = 0.5),
    X_mi = ifelse(X == 1, # New version of X with missings (missing completely at random)
                  sample(x = c(1, NA), size = N, prob = c(1-p, p), replace = TRUE),
                  sample(x = c(0, NA), size = N, prob = c(1-p, p), replace = TRUE)),
    Blocks = case_when(X_mi == 1 ~ "one",
                       X_mi == 0 ~ "zero",
                       is.na(X_mi) == TRUE ~ "missing"),
    U = b * X + rnorm(N, mean = 0, sd = 1),
    potential_outcomes(Y ~ Z + b * X + U, conditions = list(Z = c(0, 1)))
  )

data_strategy_bl <-
  declare_assignment(Z = block_ra(blocks = Blocks),
                     Y = reveal_outcomes(Y ~ Z))
  
answer_strategy_bl <-
  declare_estimator(Y ~ Z, covariates = ~X_mi, .method = lm_lin, label = "X Blocks") # including fixed effects for blocks by inserting Blocks as a factor

design_54_N_bl <- model_blocks + inquiry_ate + data_strategy_bl + answer_strategy_bl

# Varying N
design_54_N_ra <- redesign(design_54_N_ra, N = c(100, 500, 1000))
design_54_N_bl <- redesign(design_54_N_bl, N = c(100, 500, 1000))

designs_vary_N <- append(design_54_N_ra,
                         design_54_N_bl)

names(designs_vary_N) <-
  list("N = 100 Random",
       "N = 500 Random",
       "N = 1000 Random",
       "N = 100 Blocks",
       "N = 500 Blocks",
       "N = 1000 Blocks")
```

```{r}
designs_vary_N %>% 
  diagnose_design() %>%
  reshape_diagnosis(exclude = c(
    "b", "Inquiry", "Outcome", "N Sims", "Mean Estimand", "Bias", "SD Estimate", "Coverage"
  ))
```

### Varying p

```{r echo=TRUE}
p01 <- 0.1
p09 <- 0.9

# Using basic model components with N = 100
design_54_p05_ra <- model_random + inquiry_ate + data_strategy_ra + answer_strategy_ra
design_54_p05_bl <- model_blocks + inquiry_ate + data_strategy_bl + answer_strategy_bl

# New models with p = 0.1
model_random_p01 <- # random assignment
  declare_model(
    N = 100,
    b = 0.5,
    X = rbinom(N, size = 1, prob = 0.5),
    X_mi = ifelse(X == 1, # New version of X with missings (missing completely at random)
                  sample(x = c(1, NA), size = N, prob = c(1-p01, p01), replace = TRUE),
                  sample(x = c(0, NA), size = N, prob = c(1-p01, p01), replace = TRUE)),
    U = b * X + rnorm(N, mean = 0, sd = 1),
    potential_outcomes(Y ~ Z + b * X + U, conditions = list(Z = c(0, 1)))
  )

design_54_p01_ra <- model_random_p01 + inquiry_ate + data_strategy_ra + answer_strategy_ra

model_blocks_p01 <-
  declare_model(
    N = 100,
    b = 0.5,
    X = rbinom(N, size = 1, prob = 0.5),
    X_mi = ifelse(X == 1, # New version of X with missings (missing completely at random)
                     sample(x = c(1, NA), size = N, prob = c(1-p01, p01), replace = TRUE),
                     sample(x = c(0, NA), size = N, prob = c(1-p01, p01), replace = TRUE)),
    Blocks = case_when(X_mi == 1 ~ "one",
                         X_mi == 0 ~ "zero",
                         is.na(X_mi) == TRUE ~ "missing"),
    U = b * X + rnorm(N, mean = 0, sd = 1),
    potential_outcomes(Y ~ Z + b * X + U, conditions = list(Z = c(0, 1)))
  )

design_54_p01_bl <- model_blocks_p01 + inquiry_ate + data_strategy_bl + answer_strategy_bl

# New models with p = 0.9
model_random_p09 <- # random assignment
  declare_model(
    N = 100,
    b = 0.5,
    X = rbinom(N, size = 1, prob = 0.5),
    X_mi = ifelse(X == 1, # New version of X with missings (missing completely at random)
                  sample(x = c(1, NA), size = N, prob = c(1-p09, p09), replace = TRUE),
                  sample(x = c(0, NA), size = N, prob = c(1-p09, p09), replace = TRUE)),
    U = b * X + rnorm(N, mean = 0, sd = 1),
    potential_outcomes(Y ~ Z + b * X + U, conditions = list(Z = c(0, 1)))
  )

design_54_p09_ra <- model_random_p09 + inquiry_ate + data_strategy_ra + answer_strategy_ra

model_blocks_p09 <-
  declare_model( # block assignment
    N = 100,
    b = 0.5,
    X = rbinom(N, size = 1, prob = 0.5),
    X_mi = ifelse(X == 1, # New version of X with missings (missing completely at random)
                     sample(x = c(1, NA), size = N, prob = c(1-p09, p09), replace = TRUE),
                     sample(x = c(0, NA), size = N, prob = c(1-p09, p09), replace = TRUE)),
    Blocks = case_when(X_mi == 1 ~ "one",
                         X_mi == 0 ~ "zero",
                         is.na(X_mi) == TRUE ~ "missing"),
    U = b * X + rnorm(N, mean = 0, sd = 1),
    potential_outcomes(Y ~ Z + b * X + U, conditions = list(Z = c(0, 1)))
  )

design_54_p09_bl <- model_blocks_p09 + inquiry_ate + data_strategy_bl + answer_strategy_bl

designs_vary_p <- list("p = 0.1 random" = design_54_p01_ra,
                       "p = 0.1 block" = design_54_p01_bl,
                       "p = 0.5 random" = design_54_p05_ra, 
                       "p = 0.5 block" = design_54_p05_bl,
                       "p = 0.9 random" = design_54_p09_ra,
                       "p = 0.9 block" = design_54_p09_bl)
```

```{r}
#| warning: false

designs_vary_p %>% 
  diagnose_design() %>%
  reshape_diagnosis(exclude = c(
    "b", "Inquiry", "Outcome", "N Sims", "Mean Estimand", "Bias", "SD Estimate", "Coverage"
  ))
```

### Varying b

```{r echo=TRUE}
# Using basic model components with N = 100
design_54_b_ra <- model_random + inquiry_ate + data_strategy_ra + answer_strategy_ra
design_54_b_bl <- model_blocks + inquiry_ate + data_strategy_bl + answer_strategy_bl

# Varying b
design_54_b_ra <- redesign(design_54_b_ra, b = c(0.1, 0.5, 0.9))
design_54_b_bl <- redesign(design_54_b_bl, b = c(0.1, 0.5, 0.9))

designs_vary_b <- append(design_54_b_ra,
                         design_54_b_bl)

names(designs_vary_b) <-
  list("b = 0.1 Random",
       "b = 0.5 Random",
       "b = 0.9 Random",
       "b = 0.1 Blocks",
       "b = 0.5 Blocks",
       "b = 0.9 Blocks")
```

```{r}
#| warning: false

designs_vary_b %>% 
  diagnose_design() %>%
  reshape_diagnosis(exclude = c(
    "b", "Inquiry", "Outcome", "N Sims", "Mean Estimand", "Bias", "SD Estimate", "Coverage"
  ))
```
